.\"  Copyright (C) 2007 Red Hat, Inc.  All rights reserved.
.\"  
.\"  This copyrighted material is made available to anyone wishing to use,
.\"  modify, copy, or redistribute it subject to the terms and conditions
.\"  of the GNU General Public License v.2.

.TH dlm_controld 8

.SH NAME
dlm_controld - daemon that configures dlm according to cluster events

.SH SYNOPSIS
.B
dlm_controld
[\fIOPTION\fR]...

.SH DESCRIPTION
The dlm lives in the kernel, and the cluster infrastructure (cluster
membership and group management) lives in user space.  The dlm in the
kernel needs to adjust/recover for certain cluster events.  It's the job
of dlm_controld to receive these events and reconfigure the kernel dlm as
needed.  dlm_controld controls and configures the dlm through sysfs and
configfs files that are considered dlm-internal interfaces; not a general
API/ABI.

The dlm also exports lock state through debugfs so that dlm_controld can
implement deadlock detection in user space.

.SH CONFIGURATION FILE

Optional cluster.conf settings are placed in the <dlm> section.

.SS Global settings
The network
.I protocol
can be set to "tcp" or "sctp".  The default is tcp.

  <dlm protocol="tcp"/>

After waiting
.I timewarn
centiseconds, the dlm will emit a warning via netlink.  This only applies
to lockspaces created with the DLM_LSFL_TIMEWARN flag, and is used for
deadlock detection.  The default is 500 (5 seconds).

  <dlm timewarn="500"/>

DLM kernel debug messages can be enabled by setting
.I log_debug
to 1.  The default is 0.

  <dlm log_debug="0"/>

.SS Disabling resource directory

Lockspaces usually use a resource directory to keep track of which node is
the master of each resource.  The dlm can operate without the resource
directory, though, by statically assigning the master of a resource using
a hash of the resource name.

  <dlm>
    <lockspace name="foo" nodir="1">
  </dlm>

.SS Lock-server configuration

The nodir setting can be combined with node weights to create a
configuration where select node(s) are the master of all resources/locks.
These "master" nodes can be viewed as "lock servers" for the other nodes.

  <dlm>
    <lockspace name="foo" nodir="1">
      <master name="node01"/>
    </lockspace>
  </dlm>

or,

  <dlm>
    <lockspace name="foo" nodir="1">
      <master name="node01"/>
      <master name="node02"/>
    </lockspace>
  </dlm>

Lock management will be partitioned among the available masters.  There
can be any number of masters defined.  The designated master nodes will
master all resources/locks (according to the resource name hash).  When no
masters are members of the lockspace, then the nodes revert to the common
fully-distributed configuration.  Recovery is faster, with little
disruption, when a non-master node joins/leaves.

There is no special mode in the dlm for this lock server configuration,
it's just a natural consequence of combining the "nodir" option with node
weights.  When a lockspace has master nodes defined, the master has a
default weight of 1 and all non-master nodes have weight of 0.  Explicit
non-zero weights can also be assigned to master nodes, e.g.

  <dlm>
    <lockspace name="foo" nodir="1">
      <master name="node01" weight="2"/>
      <master name="node02" weight="1"/>
    </lockspace>
  </dlm>

In which case node01 will master 2/3 of the total resources and node2 will
master the other 1/3.


.SH OPTIONS
.TP
\fB-d\fP <num>
Enable (1) or disable (0) the deadlock detection code.
.TP
\fB-D\fP
Run the daemon in the foreground and print debug statements to stdout.
.TP
\fB-K\fP
Enable kernel dlm debugging messages.
.TP
\fB-V\fP
Print the version information and exit.
.TP
\fB-h\fP 
Print out a help message describing available options, then exit.

.SH SEE ALSO
groupd(8)

