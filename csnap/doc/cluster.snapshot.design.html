<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
<head>
  <title>Clustered Snapshot Design</title>
</head>
<body>
<div style="text-align: center;"><big><big><big><br>
Clustered Snapshots<br>
Detailed Design<small><br>
</small></big></big></big></div>
<div style="text-align: center;"><big><big><big><small><br>
Revised October 6, 2004<br>
</small></big></big></big><big><big><small><br>
Daniel
Phillips, Red Hat Inc.</small></big></big><br>
<big><big><big><small><br>
</small></big></big></big>
<div style="text-align: justify;">
<ol id="mozToc">
<!--mozToc h1 1 h2 2 h3 3 h4 4 h5 5 h6 6--><li><a href="#mozTocId615751">Design
Overview</a>
    <ol>
      <li><a href="#mozTocId166506">Overview of Data
structures</a></li>
      <li><a href="#mozTocId399340">Snapshot Store Layout</a></li>
      <li><a href="#mozTocId558118">Superblock and
Metablock </a></li>
    </ol>
  </li>
  <li><a href="#mozTocId412004">Client-Server Messages</a>
    <ol>
      <li><a href="#mozTocId518793">Synchronization via
server messages </a></li>
      <li><a href="#mozTocId692447">Message Sequences </a>
        <ol>
          <li><a href="#mozTocId226061">Sequences Initiated
by an Origin Client </a></li>
          <li><a href="#mozTocId674706">Sequences Initiated
by a Snapshot Client </a></li>
          <li><a href="#mozTocId170275">Sequences Initiated
by the Snapshot Server </a></li>
        </ol>
      </li>
    </ol>
  </li>
  <li><a href="#mozTocId691886">Server Operation </a>
    <ol>
      <li><a href="#mozTocId330842">Exception Tree Format </a>
        <ol>
          <li><a href="#mozTocId268045">Leaf nodes</a></li>
          <li><a href="#mozTocId478694">Index nodes</a></li>
        </ol>
      </li>
      <li><a href="#mozTocId326876">Journal</a></li>
      <li><a href="#mozTocId852538">Durability </a></li>
      <li><a href="#mozTocId244480">Chunk Allocation
Bitmaps</a></li>
      <li><a href="#mozTocId189826">Allocation Policy</a></li>
      <li><a href="#mozTocId194475">Expanding the
Snapshot Store </a></li>
      <li><a href="#mozTocId2062">Locking</a></li>
      <li><a href="#mozTocId750451">Snapshot Deletion </a></li>
      <li><a href="#mozTocId63589">Server Statistics</a></li>
    </ol>
  </li>
  <li><a href="#mozTocId149578">Client Operation</a></li>
  <li><a href="#mozTocId986766">Integration with
Cluster Infrastructure </a>
    <ol>
      <li><a href="#mozTocId291552">Server Start</a></li>
      <li><a href="#mozTocId546783">Server Shutdown </a></li>
      <li><a href="#mozTocId763101">Failure Recognition</a>
        <ol>
          <li><a href="#mozTocId909827">Snapshot Server Failure </a></li>
          <li><a href="#mozTocId909827"> Cluster Manager
Failure </a></li>
        </ol>
      </li>
      <li><a href="#mozTocId711483">Server Restart</a></li>
      <li><a href="#mozTocId407640">Client-Server
Connections </a>
        <ol>
          <li><a href="#mozTocId423442">Initial connection</a></li>
          <li><a href="#mozTocId755654">Disconnect</a></li>
          <li><a href="#mozTocId840922">Reconnect</a></li>
        </ol>
      </li>
    </ol>
  </li>
  <li><a href="#mozTocId373989">Performance
Characteristics</a>
    <ol>
      <li><a href="#mozTocId206505">Effect of chunk size</a></li>
      <li><a href="#mozTocId700589">Effect of metadata
block size</a></li>
      <li><a href="#mozTocId473026">Effect of Holding
Multiple Snapshots</a></li>
      <li><a href="#mozTocId217737">Assumptions</a></li>
      <li><a href="#mozTocId495690">Origin Read
Performance</a></li>
      <li><a href="#mozTocId519978">Sequential Origin
Write Performance</a></li>
      <li><a href="#mozTocId245882">Random Origin Write
Performance</a></li>
      <li><a href="#mozTocId983353">Snapshot Read
Performance</a></li>
      <li><a href="#mozTocId713768">Snapshot Write
Performance</a></li>
      <li><a href="#mozTocId899929">Network Performance</a></li>
      <li><a href="#mozTocId163638">Overall Performance</a></li>
    </ol>
  </li>
  <li><a href="#mozTocId895406">Parallelizing the
Architecture</a></li>
  <li><a href="#mozTocId426297">Adaptation to Single Node Client</a></li>
</ol>
</div>
<big><big><big><small></small></big></big></big></div>
<h1><a class="mozTocH1" name="mozTocId615751"></a>Design Overview</h1>
The required functionality of the clustered snapshot target, documented
in detail elsewhere, is briefly reviewed here.&nbsp; This target
implements a virtual block device that sits on top of some other block
device, allowing the creation of multiple writable snapshots of the
state of the underlying device.&nbsp; Each snapshot is also a virtual
block device implemented by the target.&nbsp; Multiple snapshots and
the origin device can be active simultaneously on multiple nodes of a
cluster.&nbsp; These virtual devices must act like real devices, that
is, they must be durable in the sense that once data is written to the
origin or to a snapshot it will never be lost even in the event of a
system crash or power outage.&nbsp; Performance of the virtual devices
must not be too much less than the underlying device.&nbsp; To save
space, snapshots must share data chunks with the origin volume and with
each other as much as possible.<br>
<br>
This design implements a client-server architecture
where almost everything interesting happens in the server.&nbsp; For a
write request to the origin, the client sends a message to the snapshot
server instructing it to ensure that the write will not interfere with
any snapshot by copying data from the origin to snapshot store if
necessary.&nbsp; The server signals that it has accomplished this by
acknowledging the message, and the client proceeds to carry out the
write.&nbsp; A snapshot client handles writes in a similar way with the
difference that the server informs the client in its response which of
the chunks the client wishes to write to are located in the snapshot
store, and where.&nbsp; Snapshot reads require some special
handling.&nbsp; The snapshot client sends a message to the server
indicating which chunks it wishes to read.&nbsp; The server locks any
of those chunks that lie on the origin and thus are at risk of being
overwritten by a client simultaneously active on the origin.&nbsp; The
server informs the snapshot client which chunks it has locked, and
after reading them, the client sends a message to the server that the
chunks may be unlocked.&nbsp; This interaction between client and
server via messages provides all the synchronization necessary to
ensure that multiple simultaneously active origin and snapshot clients
do not interfere with each other, preserving the illusion that these
virtual targets are real devices.<br>
<br>
There is a fair amount of mechanism behind the scenes in order for the
server to carry out the
work required by the above messages faithfully and efficiently.&nbsp;
This mechanism is implemented in the metadata server, the main focus of
this design document.<br>
<br>
The metadata server maintains the current state of all snapshots in a
single disk-based btree.&nbsp; The btree
permits
a variable number of exceptions to be stored per chunk.&nbsp; Within
btree leaf nodes, bitmaps are used to to record the sharing of snapshot
store data.&nbsp; Each bitmap is the same size as a logical address, 64
bits, giving a maximum of 64 simultaneous snapshots.&nbsp; Btree nodes
are operated on directly as primary data, as 64 bit alignment of
objects within the nodes is considered desireable for efficiency and
to support the stringent alignment requirements of some architectures.<br>
<br>
Free space within the snapshot store is tracked with
bitmaps with a granularity of one bit per chunk.&nbsp; Metadata
structures&nbsp; on the other hand may have a finer granularity than a
chunk, typically 4K, which is the current limitation on the size of a
kernel buffer.&nbsp; When possible, metadata allocations will be made
metadata_blocksize / chunksize blocks at a time; where this is not
possible, the system must track partially allocated chunks.&nbsp; The
current plan is to remember only the most recent partially allocated
chunk and to do metadata allocations from it until it is completely
filled.&nbsp; (This simple approach limits the ability to optimize
metadata layout by setting allocation goal, so this strategy needs to
be looked at critically.)<br>
<br>
A journal is used for durable/atomic updating of snapshot
metadata.<br>
<br>
&nbsp; - Changes to the superblock<br>
&nbsp; - Changes to the metaroot<br>
&nbsp; - Current state of the BTree<br>
&nbsp; - State of all partially allocated chunks<br>
<br>
Each origin client caches a bitmap indicating which origin chunks are
known not to be shared by any snapshot, and thus can be written without
synchronization.&nbsp; Each snapshot client similarly caches a table of
exception store addresses of chunks that are known not to be shared.<br>
<h2><a class="mozTocH2" name="mozTocId166506"></a>Overview of Data
structures</h2>
This section describes the main data structures used by the server and
client, to provide context for the detailed discussions below.<br>
<br>
Data structures used by the server are disk-resident and partially
cached in memory.&nbsp; Client caches are memory-resident.<br>
<ul>
  <li>Server On-disk Structures<br>
    <br>
  </li>
  <ul>
    <li>Superblock and Metaroot<br>
Static global data, e.g., chunk size; location of the root of the
exception btree and journal; size of the origin volume and snapshot
store; allocation bitmap stride and base<br>
    </li>
    <li>Metablock<br>
Miscellaneous variable data, e.g., snapshot list;
snapshot deletes in progress; list of chunks allocated for metadata but
only partially
used; freespace total [do we really need this?]<br>
    </li>
    <li>Journal<br>
For atomic updating and durability, changes to the exception btree,
allocation bitmaps and
miscellaneous other structures are written first to the journal then to
their final destinations.&nbsp; Without such measures, snapshot virtual
volumes would be vulnerable to disk corruption in the event of
unexpected system failure, which is undesirable and unlike a physical
volume.<br>
    </li>
    <li>Allocation bitmaps<br>
Free chunks in the snapshot store are tracked via bitmaps located just
about the journal in the snapshot store, indexed via a full-populated
radix tree<br>
    </li>
    <li>Exception Btree<br>
Indexed by chunk address; for each chunk address that has exceptions,
stores a list of exceptions.&nbsp; Each exception is paired with a
bitmap indicating which snapshots share the exception<br>
      <br>
    </li>
  </ul>
  <li>Client Cache<br>
    <br>
  </li>
  <ul>
    <li>Unique bitmap (for origin client)<br>
Each one bit in this bitmap indicates that all snapshots have
exceptions for the given chunk and so the client may write the chunk
without interacting the snapshot server.&nbsp; A zero means the chunk
is shared or its status is unknown, in either case the client must<br>
    </li>
    <li>Unique exception map (for snapshot client)<br>
Each exception in the table may be zero if its state is unknown or the
chunk does not have an exception, or the sector address of an exception
store chunk, with the low bit one if the chunk is known&nbsp; not to be
shared by any other snapshot.</li>
  </ul>
</ul>
[need a diagram]
<h2><a class="mozTocH2" name="mozTocId399340"></a>Snapshot Store Layout</h2>
From low to high disk
address:<br>
<br>
[ superblock ]<br>
[ fixed size journal ]<br>
[ bitmaps, btree leaves and nodes ]<br>
[ exceptions ]<br>
<br>
[need a diagram]<br>
<br>
The dividing line between metadata and exceptions is not fixed, but
rather is determined by allocation policy.<span
 style="font-weight: bold;"><br>
</span>
<h2><a name="mozTocId558118" class="mozTocH2"></a>Superblock and
Metablock<br>
</h2>
The first 4K block in the snapshot store is the superblock, containing
global information for the volume set at creation time:<br>
<ul>
  <li>Version<br>
  </li>
  <li>Size of snapshot store</li>
  <li>Metadata block size (4K)<br>
  </li>
  <li>Chunk size (binary multiple of metadata block size)<br>
  </li>
  <li>Sector address of the metablock</li>
  <li>Sector address of root of allocation bitmap radix tree</li>
  <li>Sector address of beginning of journal</li>
  <li>Size of journal<br>
  </li>
  <li>...?<br>
  </li>
</ul>
The metablock contains variable global information:<br>
<ul>
  <li>Status flags</li>
  <ul>
    <li>Journal clean bit<br>
    </li>
  </ul>
  <li>Highwater mark of chunk allocation</li>
  <li>Total free space<br>
  </li>
  <li>Sector address and allocated size of partially allocated chunk<br>
  </li>
  <li>Bitmask of currently active snapshots</li>
  <li>Bitmask of snapshots in process of being deleted</li>
  <li>list of sector addresses of roots of snapshot store btrees</li>
  <li>...?</li>
</ul>
Metablock data items that change frequently such as the highwater mark,
freespace and partial allocation are also recorded in the journal tag
block each time a journal transaction is committed.&nbsp; Updates to
the metablock are always journalled.<br>
<h1><a class="mozTocH1" name="mozTocId412004"></a><span
 style="font-weight: bold;">Client-Server Messages</span></h1>
<h2><a class="mozTocH2" name="mozTocId518793"></a>Synchronization via
server messages<br>
</h2>
Some form of synchronization is required in order to make clustered
origin volumes and snapshot devices act as though they are independent
devices even when they share data.&nbsp; The obvious approach would be
to use
a cluster lock manager, with shared locks for reads and exclusive locks
for writes.&nbsp; But once a client has taken a lock for a write to a
given chunk it needs to find out from the metadata server whether the
original data of the chunk needs to be preserved in the snapshot store
in the case of a write to the origin or a new exception needs to be
created in the case of a write to a snapshot.&nbsp; This generates more
message traffic than necessary; it turns out that server messages alone
are sufficient for synchronization.&nbsp; The following fact is helpful
in reducing the number of messages required: if a chunk is known to be
unshared then it can be freely written or read without global
synchronization.&nbsp; Furthermore, once an origin chunk becomes
unshared it can only become shared again by creating a new
snapshot.&nbsp; And once a chunk shared by more than one snapshot
becomes unshared it will remain unshared until the snapshot is
deleted.&nbsp; This property means that once a client is told that a
given chunk is unshared it can rely on that information for a long
period,
or more precisely, until a new snapshot is created.&nbsp; A consequence
of this is that all origin clients must clear their bitmap cache each
time a new snapshot is created; a server-initiated request/response
sequence is provided for that purpose.&nbsp; (Snapshot clients on the
other hand do not have to clear their cache because snapshot chunks
known to be unique reside in the snapshot store, thus cannot become
shared with a new snapshot.&nbsp; This is yet another reason why
snapshot IO performance is expected to ultimately supercede origin IO
performance.&nbsp; [show why snapshot creation doesn't interfere with
outstanding snapshot reads])<br>
<br>
This design is therefore based on the principle that a client will
send a request to the snapshot server for every chunk that it does not
know is unshared.&nbsp; When the client receives the response it knows
that all requested chunks are unshared.&nbsp; The server has either
determined this by consulting the btree or made it so by creating new
exceptions.&nbsp; The
client then caches the information that the chunks are unshared (in a
bitmap) to optimize the case of repeated accesses to the same
chunk.&nbsp;&nbsp; Once a chunk is known to be unshared the client can
proceed with a write operation to the chunk.<br>
<br>
Special synchronization is required for snapshot reads, to prevent an
origin chunk that is read via a snapshot from being overwritten by a
write via the origin.&nbsp; Locking is used here, however the locking
is
internal to the snapshot server, except that a snapshot client must
send messages to the server to unlock chunks that the server has locked
on behalf of the client (only for snapshot reads, not for writes or
origin reads).&nbsp; Thus the snapshot server acts as a minimal lock
manager in the case of snapshot reads.<br>
<br>
A complete enumeration of cases should clarify the above logic:<br>
<ul>
  <li><span style="font-weight: bold;">Origin write to unshared chunk</span><br>
A write to an origin chunk that is not shared (i.e., all snapshots
already have exceptions for the chunk) does not require any global
synchronization; it is the responsibility of the higher level
application to ensure that no other reads or writes race with this
write.<br>
    <br>
  </li>
  <li><span style="font-weight: bold;">Origin write to shared chunk</span><br>
A write to an origin chunk that is shared requires global
synchronization.&nbsp; This synchronization is accomplished by sending
a message to the snapshot server which will ensure that the chunk is
not shared, by examining the exception btree and creating a new
exception if necessary.&nbsp; When the client receives the reply the
chunk is gauranteed not to be shared, which reduces to the case above
and the write can proceed.&nbsp; When the client doesn't know whether a
chunk is shared or unshared it must ask the server, so "unknown" is
treated the same as "shared" by the client; once the server responds
the chunk is known to be unshared and the client can cache this
information; the chunk can only become shared again if a new snapshot
is set, in which case the client will discard any sharing information
it has cached.<br>
    <br>
  </li>
  <li><span style="font-weight: bold;">Origin read from shared or
unshared chunk</span><br>
Reads from the origin do not require any global synchronization because
the higher level application has the responsibility of ensuring that
these do not race with writes to the same volume.&nbsp; Snapshot writes
do not collide with origin reads because the destination of a snapshot
write is always the snapshot store.<br>
    <br>
  </li>
  <li><span style="font-weight: bold;">Snapshot write to unshared chunk</span><br>
A write to a snapshot logical chunk that is not shared does not require
global synchronization, as for origin writes.<br>
    <br>
  </li>
  <li><span style="font-weight: bold;">Snapshot write to shared chunk</span><br>
A write to a snapshot chunk that is shared is similar to a write to the
origin except that the snapshot server must also return a set of
exception store addresses to the client, which the client caches.<br>
    <br>
  </li>
  <li><span style="font-weight: bold;">Snapshot read from unshared chunk<br>
    </span>A snapshot read from an unshared chunk does not require any
global synchronization.<br>
    <br>
  </li>
  <li><span style="font-weight: bold;">Snapshot read from shared chunk</span><br>
A snapshot read of a shared chunk requires global synchronization to
ensure that a write to the same chunk via the origin does not overwrite
the data while it is being read.&nbsp; The snapshot server performs
this synchronization by locking locally on the snapshot server between
writers and readers of shared chunks, details below.&nbsp; Chunks that
are locked for reading on a snapshot have to be unlocked after the read
is complete, which requires an additional message from the client to
the server. Similarly to writes to shared chunks, if the client doesn't
know that a chunk is shared it must contact the server.&nbsp; The
server's response indicates which chunks are unshared and the client
can cache this information.</li>
</ul>
Each chunk in the original message, once acknowledged, is guaranteed to
be unique because the metadata server has either
determined that each chunk is already unique or it has completed a copy
to snapshot store to make it unique.&nbsp; [note:&nbsp; chunks are not
necessarily acknowledged in the requested order]&nbsp; The only way a
unique chunk
can become shared is when a new snapshot is set, in fact, at the time a
new snapshot is set all its chunks are shared with at least the
origin.&nbsp; For this reason, setting a new snapshot requires that all
origin clients discard their bitmaps.&nbsp; Thus, the server sends a
"new snapshot" message to every client and thenew snap shot does not
become writeable until every client has acknowledged this message.<br>
<h2><a class="mozTocH2" name="mozTocId692447"></a>Message Sequences<br>
</h2>
This section enumerates the messages in each synchronization sequence.<br>
<h3><a class="mozTocH3" name="mozTocId226061"></a>Sequences Initiated
by an Origin Client<br>
</h3>
<ul>
  <li>Origin write</li>
  <ul>
    <li>Client sends unique request</li>
    <ul>
      <li>request gives chunk address range<br>
      </li>
    </ul>
    <li>Server responds with initiate</li>
    <ul>
      <li>empty message, i.e., "write can proceed" </li>
    </ul>
    <ul>
      <li>server has verified each chunk is unshared or created new
exceptions as necessary<br>
      </li>
      <li>all chunks are now unique so unique cache can be updated for
these chunks</li>
    </ul>
  </ul>
</ul>
<h3><a class="mozTocH3" name="mozTocId674706"></a>Sequences Initiated
by a Snapshot Client<br>
</h3>
<ul>
  <li>Snapshot write</li>
  <ul>
    <li>Client sends unique request</li>
    <ul>
      <li>request gives chunk address range<br>
      </li>
    </ul>
    <li>Client responds with initiate</li>
    <ul>
      <li>response lists exception addresses, if any</li>
      <li>after verifying each chunk is unshared, or creating new
exceptions where not</li>
    </ul>
  </ul>
</ul>
<ul>
  <li>Snapshot read</li>
  <ul>
    <li>client sends read lock request</li>
    <ul>
      <li>request chunk address range<br>
      </li>
    </ul>
    <li>Server responds with initiate</li>
    <ul>
      <li>lists exception addresses for non-origin chunks</li>
      <li>lists which chunks need to be unlocked because they are not
unique</li>
    </ul>
    <li>Client sends unlock when done</li>
    <ul>
      <li>for non-unique chunks above<br>
        <br>
      </li>
    </ul>
  </ul>
  <li>Snapshot create</li>
  <ul>
    <li>client sends snapshot create message</li>
    <li>server sends snapshot create advice to each origin client</li>
    <li>each origin client clears its bitmap cache and acknowledges
create<br>
    </li>
    <li>server returns create acknowlege</li>
    <li>(I'm not completely satisfied with this sequence)<br>
    </li>
  </ul>
</ul>
<h3><a class="mozTocH3" name="mozTocId170275"></a>Sequences Initiated
by the Snapshot Server<br>
</h3>
[snapshot create]; cluster management messages; error messages;
shutdown message; [fixme]<br>
<h1><a class="mozTocH1" name="mozTocId691886"></a>Server Operation<br>
</h1>
<h2><a class="mozTocH2" name="mozTocId330842"></a>Exception Tree Format<br>
</h2>
Exceptions for all snapshots are stored in a single btree indexed by
logical chunk address.&nbsp; For each chunk, a list of exceptions is
stored.&nbsp; Each exception consists of a snapshot address and a
bitmap indicating which snapshots share that exception.<br>
<br>
Rather than serving only as a disk format to be translated into some
more efficient cache format, the btree is meant to be operated on
directly by the snapshot server.&nbsp; To this end, data structures in
the btree nodes and leafs are designed for direct memory access, e.g.,
all numeric values are aligned according to their size.<br>
<br>
An attempt has been made to keep the btree compact by designing the
node formats carefully, without going to extremes such as using a
serial compressed encoding which is unpacked into a memory structure in
order to be accessed.&nbsp; In other words, difficult tradeoffs have
been made here between compactness, simplicity and efficiency.<br>
<h3><a class="mozTocH3" name="mozTocId268045"></a>Leaf nodes</h3>
Leaf block format is optimized for rapid lookup and efficient
insertion.&nbsp; At the bottom of each leaf is a header and a directory
map that grows up towards a table of exceptions, which grows
down.&nbsp; Each entry in the directory map gives the logical chunk
address
relative to a base address stored in the header, and has a pointer to
one of the exceptions in the table at the top of the block.&nbsp; The
entries are stored in sorted order according to logical chunk address
and the pointers increase monotonically.<br>
<br>
[need a diagram]<br>
<br>
Using relative addresses allows the map entries to be more
compact.&nbsp; In the current prototype map entries consist of two 32
bit numbers, however two 16 bit numbers might work just as well and
save more space, although a 16 bit relative block number might be so
small as to cause a noticeable increase in the number of leaf blocks if
exceptions.are distributed sparsely.&nbsp; With 32 bit map numbers, a
single exception requires 24 bytes; with 16 bit map numbers that would
fall to 20 bytes, a 16% savings.&nbsp; The final determination of which
is best should probably be determined experimentally.<br>
<br>
The difference between each two pointers in the map gives the number of
exceptions for the chunk.&nbsp; The last entry in the map is a sentinel
and points at the top of the block (this could be designed out to save
a few bytes).&nbsp; Each entry in the exception table has the 64 bit
sector address of an exception in the snapshot store and a bitmap to
indicate which snapshots share the exception.<br>
<br>
The basic operations to locate and determine sharing of exceptions are
quite efficient.&nbsp; A binary search is used to locate the target
chunk address in the map, if it is present.&nbsp; This yields a list of
exceptions on which efficient bitwise operations can be performed to
determine sharing. From the point of view of the origin, a logical
chunk is shared unless all active snapshots have exceptions for that
chunk.&nbsp; From the point of view of a snapshot, a logical chunk is
shared if it has no exception (i.e., is shared with the origin) or it
has the same snapshot store address as another snapshot.<br>
<br>
A slight drawback of this leaf format is that insertion requires memory
moves
in order to maintain the entries in sorted order, and the memory moves
get longer as the leaf block fills up.&nbsp; For relatively small leaf
blocks, i.e. 4K, it is probably not a problem.&nbsp; This will be
determined experimentally.&nbsp; Other, equivalently efficient leaf
formats are certainly possible, though perhaps they will not be as
simple.<br>
<br>
A more serious drawback of this leaf format is that as the number of
snapshots increases, update overhead of the btree will increase more or
less linearly.&nbsp; It is thus desirable to adopt a variant leaf
format at some point capable of encoding runs of adjacent exceptions
efficiently.&nbsp; [variant leaf format needs to be provided for in
per-leaf flags and in superblock flags, for backward
compatibility.]&nbsp; This issue is treated at greater length in the
performance section, below.&nbsp; In brief: this will not be a problem
for reasonable numbers of simultaneous snapshots.<br>
<h3><a class="mozTocH3" name="mozTocId478694"></a>Index nodes</h3>
An index node contains a table of entries each of which consists of a
64 bit logical chunk address key and a 64 bit sector address of a lower
level index node or, at the lowest index level, a leaf.&nbsp; The
entries are in sorted order by logical chunk address.&nbsp; Two
successive keys bound the range of entries contained by the lower level
node.<br>
<br>
To locate the leaf block in which exceptions, if any, are stored for a
given logical address, we descend recursively from the root, doing a
binary search on the address key in each block and descending
recursively into the node referenced by the sector address lying
between the two keys that bound the target key.<br>
<br>
We search all the way to a leaf node even if we are examining a region
of the address space that is completely empty.&nbsp; For write requests
this is not inefficient because we will immediately add an exception to
the&nbsp; leaf node we found if one is not present.&nbsp; For read
requests it's a little more work than necessary but we probably do not
care since this only affects snapshot reads, and only by a small amount
(origin reads do not involve the server).<br>
<h2><a class="mozTocH2" name="mozTocId326876"></a>Journal</h2>
Any altered metadata block, i.e, btree leaf and index nodes,
allocation bitmaps, etc, are written to a journal before being written
to their final destinations.&nbsp; This gaurantees that the metadata
can
be restored reliably to the state of the most recently committed
exception or other metadata change.<br>
<br>
The size and location of the journal are determined at the time the
snapshot store is created and cannot be changed.<br>
<br>
Each journal transaction consists of an arbitrary number of data blocks
followed by a journal tag block.&nbsp; The tag block carries a magic
number allowing it to be identified as such for the purpose of journal
replay, and a sequence number used to locate the starting point for
journal replay. Any data block written to the journal that happens to
have the same number at the same location must be escaped by writing a
zero to that location in a copy of the data.&nbsp; The tag block
carries a list of snapshot
store sector addresses which are the final destinations of the data
blocks.&nbsp; The low bit of the address carries a bit flag indicating
that the data block was escaped and the magic number needs to be
restored before the data block is finally written.&nbsp; The tag block
carries other miscellaneous information such as partial usage status of
a chunk recently allocated for metadata.<br>
<h2><a class="mozTocH2" name="mozTocId852538"></a>Durability<br>
</h2>
Thanks to the journal, the entire state of the metadata server (with on
exception, see below) is always completely recorded on disk at the time
any write is acknowledged.&nbsp; Thus, if the metadata server should
fail a new one can be started, read the metadata root and continue as
if nothing had happened.<br>
<br>
The one exception to this is that locking state of snapshot read
requests against origin writes is kept only in memory on the
server.&nbsp; While it is enough to simply requre that all outstanding
reads on clients must complete before a newly started metadata server
can resume processing requests, there could be cases where this would
cause an unnecessary delay of serveral seconds on server restart where
there is a heavy backlog of IO.&nbsp; Since it is easy,
clients will be asked to upload any outstanding locked snapshot reads
to the new metadata server before the server resumes processing
requests.&nbsp; This should only take a few tens of milliseconds.&nbsp;
The total latency of starting a new metadata server then should be
measured in tens of milliseconds (though detecting that a server has
failed could easily take much longer).<br>
<br>
[more details of journalling]<br>
[for the kernel implementation, considerations for continued access to
metadata blocks that are currently in the journal writeout]<br>
<h2><a class="mozTocH2" name="mozTocId244480"></a>Chunk Allocation
Bitmaps</h2>
Freespace in the snapshot store is mangaged via bitmaps with a
resolution of one bit per chunk.&nbsp; Each bitmap is one 4K block in
size and maps 2**15 chunks.&nbsp; The bitmap blocks are indexed via a
radix tree rooted in the header.&nbsp; Each radix tree node contains
512&nbsp; 8-byte sector addresses.&nbsp; As a slight simplification
this tree is always 3 levels deep, giving 2^27 * 2^15 =&nbsp; 4
trillion chunks, or 16 petabytes volume size limit with a minimal 4K
chunk size.&nbsp; It is always fully populated, i.e., the tree is
created at the time the snapshot store is created and changed only if
the snapshot store is expanded.&nbsp; The second lowest level of the
bitmap index tree is
loaded into memory when the volume is activated, this will be about 512
KB per terabyte of snapshot store.<br>
<br>
Bitmaps are cached in buffers and accessed via getblk.&nbsp; A pointer
is kept to the most recently accessed bitmap, i.e., it is not released
until a different bitmap is accessed, which eliminates the majority of
getblk lookups assuming reasonably good locality of allocation.&nbsp;
Likewise, a pointer is kept to the most recently accessed index
block.&nbsp; Since nearly all accesses to bitmaps are associated with
changing the bitmap, the bitmaps are kept near the journal rather than
being distributed throughout the snapshot store.&nbsp; This is purely a
matter of allocation policy since the actual locations of bitmaps are
determined by the radix tree.<br>
<br>
Since metadata is allocated in blocks but allocation granualrity is
chunks, some chunks allocated to metadata may be only partially
full.&nbsp; To avoid leakage of this unallocated space on unexpected
restart, any partial allocations are recorded in the journal tag
block.&nbsp; As a side effect, this means that a few
metadata blocks can be allocated before a bitmap needs to be modified,
saving some journal bandwidth.<br>
<h2><a class="mozTocH2" name="mozTocId189826"></a>Allocation Policy</h2>
[coming soon]<br>
<br>
[see the performance section re why this is important]<br>
<br>
[Should there be hints re total free space in each region?&nbsp;
Probably]<br>
<h2><a class="mozTocH2" name="mozTocId194475"></a>Expanding the
Snapshot Store<br>
</h2>
To expand the snapshot store, additional bitmaps and associated radix
tree index blocks need to be allocated, hopefully not too far away from
the journal.&nbsp; Besides updating the snapshot store size in the
header, this is the only change that needs to be made (I think).<br>
<h2><a class="mozTocH2" name="mozTocId2062"></a>Locking</h2>
Synchronization via locking is only required between snapshot reads and
origin writes.&nbsp; This locking takes place entirely within the
server so no cluster lock manager
is involved.&nbsp; (In fact the server is a lock manager for the
limited case of snapshot reads.)&nbsp; The locks are simple, hashed
locks.&nbsp; The cost of this locking will be one hash lookup per
snapshot read or origin write of a shared chunk, plus the unlock
messages.&nbsp; This locking is only required when snapshot and origin
virtual devices are active at the same time; e.g., the server does not
have to take any locks to service origin write requests if no snapshot
device is active, even if snapshots are being held.<br>
<br>
The server takes a (local) lock for each shared chunk in the range of a
client snapshot read request, if the chunk has no exception for that
snapshot and therefore might collide with a write to the origin.&nbsp;
Locked chunks are marked in the response.&nbsp; The client sends a
message to release the lock after it has completed the read.&nbsp;
Meanwhile, the server briefly locks each chunk of a client's write
request after completing the copy to snapshot store and recording the
new exception, but before allowing the actual write to proceed by
replying to the client's request.&nbsp; This ensures that a contending
read always completes before the write to the origin takes place or is
initiated after the new exception has been recorded, thus directing the
read to the snapshot store instead of the origin.<br>
<h2><a class="mozTocH2" name="mozTocId750451"></a>Snapshot Deletion </h2>
Because it packs together information for multiple
snapshots in each leaf node, the exception btree is optimized for
lookup and exception
insertion as it should be.&nbsp; However, snapshot deletion is not as
simple an operation&nbsp; as it would be if each snapshot had its own
tree.&nbsp; (But if each snapshot had its own tree then exception
creation time would increase with the number of snapshots, much more
space would be used for multiple snapshots and keeping track of
exception sharing would be less efficient.)&nbsp; In general, deleting
a snapshot requires examining the entire btree and modifying each
leaf&nbsp; block that contains an exception for the snapshot.&nbsp;
This could amount to quite a lot of IO traffic and take a significant
amount of time.&nbsp; The snapshot server will therefore simply log the
status of the snapshot as "in process of deleting" and indicate
completion immediately to the requesting client.&nbsp; The actual
deletion will proceed in the background.&nbsp; When the deletion is
finished, which could require tens of seconds for a large volume, the
snapshot is logged as available for reuse.<br>
<br>
A possible optimization is to defer deletions until several snapshots
can be deleted in one pass, which will require less time than deleting
each individually.&nbsp; How much less depends on how common it is for
exceptions of several snapshots being deleted to lie in the same btree
node.&nbsp; Another possible optimization is to include in each index
node a bitmap indicating which snapshots have exceptions in the subtree
descending from that node so that entire subtrees can be skipped during
the traversal if they do not need to be modified.<br>
<br>
A more aggressive and considerably more difficult optimization would
involve introducing the concept of snapshot set generations and tagging
each leaf block with a the snapshot generation as of the most recent
alteration.&nbsp; Then a snapshot could be deleted by creating a new
generation that does not include the deleted snapshot.&nbsp; A leaf
block tagged with an earlier generation would be seen as "stale" and
would be modified when next encountered, to remap it to the current
generation, removing exceptions belonging to deleted snapshots in the
process.&nbsp; The complexity of this approach makes it unattractive,
however if snapshot deletion performance turns out to be a problem it
could turn out to be worth the effort.<br>
<h2><a class="mozTocH2" name="mozTocId63589"></a>Server Statistics</h2>
The current count of free chunks in the snapshot store is recorded as a
64 bit value in the journal tag block.&nbsp; In the event of unexpected
restart this value will be exact since it records the value as of the
most recent commit, which is the state recovered by replaying the
journal.<br>
<h1><a class="mozTocH1" name="mozTocId149578"></a>Client Operation</h1>
[this section was pasted in from the "barebones client specs" I gave to
Patrick and needs rewriting]<br>
<br>
Client operation is simple: all information required to map an incoming
request to its destination is obtained from the server, so the clients
just need to implement some simple message handling and a cache, the
latter not being essential for correct operations.<br>
<br>
Client initialization:<br>
<br>
&nbsp; Our target only needs to know three things from the outside
world:<br>
<br>
&nbsp;&nbsp;&nbsp; - device<br>
&nbsp;&nbsp;&nbsp; - socket<br>
&nbsp;&nbsp;&nbsp; - chunk size<br>
<br>
&nbsp; Later, snapshot clients will need to be tied to the origin
client in an<br>
&nbsp; as yet unidentified way.<br>
<br>
&nbsp; On initialization the target starts a kernel thread to handle
server<br>
&nbsp; responses.<br>
<br>
Read/write request handling:<br>
<br>
&nbsp; - Each read request is identity-mapped and dm takes care of
submitting it.<br>
<br>
&nbsp; - The target places each write request on a deferred list and
sends a<br>
&nbsp;&nbsp;&nbsp; "prepare write" request to the server.&nbsp; The
prepare write message<br>
&nbsp;&nbsp;&nbsp; contains a single range of chunk addresses (for now,
later we will add<br>
&nbsp;&nbsp;&nbsp; request batching) which the server will make
unique.&nbsp; This range covers<br>
&nbsp;&nbsp;&nbsp; the sector range of the corresponding deferred write
request.<br>
<br>
&nbsp; - For each "prepare write" response received from the server the
target<br>
&nbsp;&nbsp;&nbsp; searches the deferred write list for a request with
the indicated<br>
&nbsp;&nbsp;&nbsp; chunk address, verifies that the chunk count
matches, removes it from<br>
&nbsp;&nbsp;&nbsp; the list and submits it.<br>
<br>
Other messages:<br>
<br>
&nbsp; - We don't need to handle any other messages for now.&nbsp;
Later we will add<br>
&nbsp;&nbsp;&nbsp; a variant for handling snapshot prepare write
messages and the three-step<br>
&nbsp;&nbsp;&nbsp; snapshot read messages.&nbsp; Later, there will be a
snapshot creation message<br>
&nbsp;&nbsp;&nbsp; that allows origin clients to discard their 'unique'
cache.<br>
<br>
Message formats:<br>
<br>
&nbsp; - Messages are in network byte order, halfword aligned.<br>
<br>
&nbsp; - Message header:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; be_u16 magic;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; be_u16 msg_id;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; be_u32 msg_len;<br>
<br>
Prepare write message:<br>
<br>
&nbsp; header;<br>
&nbsp; be_u16 num_ranges; /* always 1 for now */<br>
&nbsp; be_u64 chunk_address;<br>
&nbsp; be_u16 chunk_count;<br>
<br>
Prepare write response:<br>
&nbsp; header;<br>
&nbsp; be_u16 num_ranges; /* always 1 for now */<br>
&nbsp; be_u64 chunk_address;<br>
&nbsp; be_u16 chunk_count;<br>
<br>
- Matching chunk addresses to blocked writes<br>
<br>
- Caching in client is most of it, and it's optional<br>
<h1><a class="mozTocH1" name="mozTocId986766"></a>Integration with
Cluster Infrastructure</h1>
A snapshot client's entire interface with cluster infrastructure takes
place over a named socket connection to a user space daemon, the csnap
"agent".&nbsp; This agent is a fairly lightweight program with a simple
interface that is intended to be customized to a particular operating
environment.<br>
<br>
At present, the only traffic between the device mapper target and the
user space agent is a three message sequence:<br>
<ol>
  <li>Client requests a&nbsp; server connection</li>
  <li>Agent responds by opening and passing a server connection to the
client</li>
  <li>Client confirms that it successfully established communication
with the server. </li>
</ol>
For consistency, these messages are all in the same format as messages
between the client and server, except for 2. which additionally uses a
file descriptor passing interface operating over the local socket (see
below).<br>
<br>
The general scheme for snapshot server failover is:<br>
<ul>
  <li>The client loses its connection</li>
  <li>The client tells the userspace service daemon that it lost its
connection</li>
  <li>The service daemon contacts rgmanager, or service manager, or
some definitive source, and asks, "where is the server?"</li>
  <li>If there is a server, then the client simply tries to reconnect
to the existing server.</li>
  <li>If not, then rgmanager will start one, somewhere.</li>
  <li>After successfully connecting to a new snapshot server, the
service daemon passes the connection to the client<br>
  </li>
</ul>
(Thanks for expressing this succinctly, Ben)<br>
<br>
If a newly started server has to recover any global state from a
client, then it must interface with the connection manager of the
cluster infrastructure to be sure that all clients possessing global
state that were connected to the previous server incarnation have
either reconnected or left the cluster.&nbsp;&nbsp; [This interface is
under construction]<br>
<br>
A pleasant property of this cluster snapshot design is that, by adding
an additional message to the snapshot client read protocol, the need to
recover global state is eliminated, and so is the need to devise a
connection manager interface protocol.&nbsp; However, the connection
manager interface protocol in itself is an interesting and useful
design excercise, so the current plan is to implement both options.<br>
<h2>Server Infrastructure Interface</h2>
[under construction]<br>
<br>
- needs something to start it<br>
&nbsp;&nbsp; - which tells it<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - local name of origin device<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - local name of snapshot store<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - port to bind to<br>
- that something needs to know only one is started<br>
<h2>Client Infrastructure Interface<br>
</h2>
<h3>Client Connection to Server <br>
</h3>
A snapshot or origin client takes the form of a device mapper target
module, which is a standard kernel module that registers itself with
device mapper at the time it is initialized, either when the module is
inserted, or at kernel bootstrap time if the module is built into the
kernel.&nbsp; The csnap module registers a set of methods with device
mapper, one of which is the "ctr" (constructor) method.<br>
<br>
Device mapper can be directed to instantiate a csnap virtual device
directly via the device mapper ioctl interface (ioctl on the
/dev/mapper/control device) or indirectly via libdevmapper, or by
higher level utilities such as dmsetup, LVM2 or EVMS, which use
libdevmapper, which in turn uses the ioctl interface.&nbsp; To
instantiate
the device, device mapper invokes the csnap constructor, passing it a
set of ascii parameters in much the same form as a C main
routine.&nbsp; The csnap target expects the
following parameters:<br>
<ul>
  <li>Snapshot number (-1 = origin)</li>
  <li>Name of the origin physical device<br>
  </li>
  <li>Name of the snapshot store physical device<br>
  </li>
  <li>Name of a control socket</li>
</ul>
The csnap target opens the two "physical" devices (which may themselves
be virtual devices) and the control socket.&nbsp; If everything is in
order, initialization completes by sending a message over the control
socket to request a snapshot server connection.&nbsp; The contructor
method returns success and device mapper dutifully enters the name of
the virtual device into the /dev/mapper directory.<br>
<br>
At this point the device exists and may receive IO requests, but any
request received before a server connection is established will merely
be queued for later processing.&nbsp; A user space agent
establishes the server connection and passes it to the csnap target
over the named control socket.&nbsp; To handle failover,
the agent monitors the control socket connection for
further server connection requests from the csnap target.<br>
<br>
Eventually, the csnap target will receive a server connection over its
control socket via a fd-passing interface.&nbsp; It then identifies
itself to the server, and supplies the number of the snapshot it wishs
to access.&nbsp; The server responds with an error if that snapshot
does not exist (do we want to have an option to force snapshot creation
on device initialization, or do we always want to do
this?).&nbsp;&nbsp; If the server indicates that it is satisfied with
its new client, the csnap target sends a success message over its
control socket and proceeds to process any queued IO requests.
<h3>Client Reconnect<br>
</h3>
The client may lose its server connection for one of several reasons:<br>
<ol>
  <li>The server fails</li>
  <li>The connection fails</li>
  <li>The client sends an erroneous message and the server closes the
connection</li>
  <li>The client detects an erroneous reponse from the server or does
not receive a response</li>
</ol>
All these cases are handled the same way: by sending an error message
over the control socket, then requesting a new connection as for
initialization.&nbsp; When (if) the target receives a new connection,
any outstanding requests will be retried starting from the beginning of
the relevant protocol.&nbsp; In the case of snapshot reads or writes,
the original query and the actual IO have to be retried, in case a
newly instantiated snapshot server has allowed some other client to
overwrite the target data blocks.<br>
<br>
The client cannot know the difference between cases 1 and 2, and in any
case, it handles the two the same way.&nbsp; The external control
program will try to determine only the client's connection has failed,
and reestablish it, or if the server has failed, in which case a new
one has to be started.&nbsp; It will rely on a higher level service in
the cluster for these determinations, the end result of which should be
a new server connection, or an unrecoverable error.&nbsp; In cases 3
and 4, the external control program may decide not to supply a new
connection but to place the client into an error state instead, so that
all pending and future IO requests will be failed.<br>
<br>
There are two flavors of snapshot read protocol: the "3 message" and
the "4 message" protocol.&nbsp; The latter provides a confirmation that
a client's unlock message has been processed by the server, so the
client knows that a newly instantiated server has not allowed any other
client to overwrite the data it was reading.&nbsp; In contract, if the
3 message protocol is used, there is no confirmation of an unlock and
the client must upload the list of read locks it thinks it holds to the
new server before the new server will reply to origin write queries on
behalf of any client.&nbsp; So with the 4 message protocol there is no
special recovery work for the client to do on server failover, whereas
with the 3 message protocol, a list of read locks may have to be
uploaded.<br>
<br>
The 4 message protocol is clearly the more robust of the two, though
only the 3 message protocol is currently implemented.<br>
<h3><a class="mozTocH2" name="mozTocId763101"></a>Client-side Error
Detection</h3>
Currently, the client only detects errors in server reply message
syntax, which it reports to its user space agent.&nbsp; Traditionally,
detection of liveness of a server is handled by a cluster heartbeating
system.&nbsp; However, there are some errors that such a heartbeating
system will not detect, such as a stuck request, which the server
receives but never responds to, or a case where the client's connection
to the server breaks while the heatbeat system's does not.&nbsp; For
robustness, it is planned to incorporate additional timing-based error
detection into the client, including:<br>
<ul>
  <li>Stuck request detection</li>
  <li>Liveness of server connection</li>
</ul>
Because the client is designed to fail over transparently, it is
acceptable to make a pessimistic determination of failure, where a
response that is simply slow is treated as if it will never
arrive.&nbsp; In either case, the client will report the error to its
user space agent&nbsp; and wait for instructions, meanwhile attempting
to continue processing IO requests.&nbsp; The agent in turn will pass
the error on to a higher level: to a cluster, to a human operator, or
both.<br>
<br>
A stuck request will typically indicate a client or server
bug.&nbsp;&nbsp; In such a case, it is highly desirable to provide
accurate fault-isolation information to a central cluster manager as
opposed to silently leaving a user application stuck in a kernel
D-state.<br>
<h3><a name="mozTocId909827" class="mozTocH2"></a> User Space Agent
Failure</h3>
The user space agent is a local process and is considered reliable, so
no failover action is attempted if fails or if the client's connection
to the agent breaks.&nbsp; The client will respond by failing all
outstanding and future IO requests.&nbsp; This situation could only
arise from a bug, administrator error or similar.&nbsp; To recover, the
device mapper device needs to be removed and recreated.<br>
<br>
One possible exception to this might be a feature to permit live
upgrade of the agent.&nbsp; The adminstrator kills the agent and starts
a new, improved agent, while the client attempts to reconnect to the
agent in a polling loop.&nbsp; Though it is not clear at this point why
anyone would need such a feature, it would be easy to provide.<br>
<h1><a class="mozTocH1" name="mozTocId373989"></a>Performance
Characteristics</h1>
<h2><a class="mozTocH2" name="mozTocId206505"></a>Effect of chunk size</h2>
Larger chunk size will help performance for sequential and hurt for
random write loads.&nbsp; The total size of metadata reduces linearly
with the chunk size, saving space, IO bandwidth and seeking. &nbsp; On
the other hand, larger chunks increase internal fragmentation of the
snapshot store, especially for sparse, random access loads, and the
overhead of metadata updating is supposed to be small in relation to
data transfers.&nbsp; Therefore it is hoped that the performance and
metadata size cost of small chunk sizes will be outweighed reduced
internal fragmentation, saving space in the snapshot store.&nbsp; This
remains to be tested in practice.<br>
<h2><a class="mozTocH2" name="mozTocId700589"></a>Effect of metadata
block size</h2>
Larger metadata blocks will improve performance somewhat on largely
serial write loads due do requiring a fewer number of larger IOs,
especially if the snapshot metadata is fragmented.&nbsp; However, for
the time being Linux does not support IO buffers larger than physical
page size, so it is expected that metadata block size will not increase
until this issue is addressed, at least for a kernel implementation of
the snapshot metadata server.&nbsp;&nbsp; For compatibility with the
expected kernel metadata server, the user space implementation will use
4K blocks.<br>
<br>
It is thought that communication overhead and server load will not be
significant performance factors, due to these being highly
optimized.&nbsp;
Contention on large clusters with parallel loads should not be a
significant factor either, since a single server should be able to
handle the traffic of many nodes of similar power to itself.&nbsp; The
exception to this is copy-out overhead which could easily saturate a
server's bus; a simple solution is available: farm out the copy-out
traffic to lightly-loaded nodes as necessary.<br>
<h2><a class="mozTocH2" name="mozTocId473026"></a>Effect of Holding
Multiple Snapshots</h2>
The more snapshots that are held, the more btree leaf nodes will be
required to hold them.&nbsp; Journalling the extra btree leaves to disk
consumes IO bandwidth, causes more seeking and generates cache
pressure.&nbsp; Reading in the extra btree nodes increases
latency.&nbsp; However, because exceptions for all snapshots are stored
adjacent in the btree, the overhead is not as large as if a separate
map had to be updated on disk for each snapshot.&nbsp; Importantly, the
process of determining whether a given chunk is shared never requires
more than a single leaf node to be examined.<br>
<br>
Sharing bitmaps are used within leaf nodes to avoid having to enter any
given snapshot store address more than once into the node, and also
performs the function of specifiying which snapshot uses a given
snapshot store address.&nbsp;&nbsp; The worst case arises when a given
logical chunk is written at least once after every snapshot.&nbsp; Then
the leaf node entries for that chunk have a bitmap and a snapshot store
address for every snapshot.&nbsp;&nbsp;&nbsp; Since leaf nodes are
expected to be 50% full in the initial implementation, we can end up
with one exception stored in each leaf node.&nbsp; Then the number of
btree nodes that have to be journalled is equal to the number of chunks
written.&nbsp; The journalled node has to be written twice, once to the
journal and once to its true destination.&nbsp; So the worst case is a
factor of 3 degradation in write performance due to btree updating
alone.&nbsp; To ameliorate such degradation it would be wise to use a
larger chunk size when large numbers of snapshots are expected.<br>
<br>
The worst case degradation above can be tempered somewhat by improving
the btree update algorithm to use a b+tree algorithm, which guarantees
2/3rds leaf fullness, enough to hold two exceptions instead of
one.&nbsp; Larger metadata blocks will help reduce seeking overhead,
when they become practical.. &nbsp; Eventually though, the best
strategy is to introduce variant leaf node formats that optimize for
the many-snapshots case by representing ranges of snapshot store chunks
compactly, especially where the snapshot store chunks are allocated
sequentially, which is something we want to achieve anyway.<br>
<br>
In brief, the metadata update component of origin and snapshot write
performance will degrade linearly with the number of
snapshots held, but with a much shallower slope than if snapshot store
data were not shared and
metadata were not grouped together by logical address.&nbsp; In the
latter case, copy-out overhead would increase directly with number of
snapshots.&nbsp;&nbsp; Exception
table update overhead would increase rapidly as well, though the exact
rate is harder to characterize because it depends on the chunk sharing
patterns.<br>
<br>
With the maximum number of snapshots held (64) the new design should
perform better than the old one
by a factor of thirty or more.&nbsp; Furthermore, some fairly
straightforward improvements to the btree leaf format can make the
slope much shallower, to the point where the overhead of holding 64
snapshots may be hard to notice.<br>
<br>
With a single snapshot held, the new design not perform quite as well
as the existing device-mapper design, but only because the existing
design does not provide durable recording of snapshot store
updates.&nbsp; In any case, the overhead of the durable snapshot
recording is expected to be only about 2% worst-case overhead vs raw
writing, far less than the 200% worst-case overhead of copy-outs when a
single snapshot is held, and shrinks roughly linearly with the chunk
size (extra seeking in the metadata region makes this relationship
slightly more complex).&nbsp; So by using a 256K chunk size, metadata
update can most likely be held to a few percent of first-time write
overhead even when the maximum number of snapshots are held.<br>
<h2><a class="mozTocH2" name="mozTocId217737"></a>Assumptions</h2>
Performance estimates below are based on the assumption that the
smallest chunk size (4K) is used.&nbsp;&nbsp; Each new exception uses
20 bytes (exception store address, sharing bitmap and directory entry)
so each btree leaf node holds a maximum of about 200 exceptions.&nbsp;
Due to splitting, leaf nodes are normally not full.&nbsp; In fact worst
case fullness of 50% is expected for the early implementations, so leaf
nodes will hold about 100 exceptions each.<br>
<br>
The performance estimates here assume asynchronous IO, which for user
space is not yet a practical possibility in Linux, therefore a kernel
implementation is assumed.&nbsp; The initial implementation however is
in user space; without asynchronous IO the user space implementation
will not perform as well as a kernel implementation.&nbsp; It is
expected that both implementations will be developed and maintained;
that the user implementation will be available first; that a kernel
implementation will supercede it in performance; and that the user
space implementation will eventually pull even with the kernel
implementation by taking advantage of newly available asynchronous IO
and user space locking facilities.<br>
<h2><a class="mozTocH2" name="mozTocId495690"></a>Origin Read
Performance</h2>
Origin reads are passed straight through to the underlying
volume.&nbsp; Since the overhead of the device mapper handling is
insignificant, origin read performance is essentially unchanged<br>
<h2><a class="mozTocH2" name="mozTocId519978"></a>Sequential Origin
Write Performance</h2>
Origin write throughput is affected mainly by the frequency of chunk
copy-outs and metadata update overhead.&nbsp;&nbsp; Copy-outs require
reading and writing, requiring a minimum of 200% additional bandwidth
vs raw write and additional seeking as well, especially for the
single-spindle case where the origin volume and snapshot store will be
far apart.&nbsp; Throughput is improved at the expense of latency by
batching the copy-out reads and copy-out writes, which happens
naturally with asynchronous IO.&nbsp; There will thus be fewer long
seeks between the origin and snapshot store.<br>
<br>
Worst case origin write performance is obtained when the snapshot store
is created with the smallest possible chunk size (4K) and the load
requires a copy-out for every chunk write.&nbsp;&nbsp; Such a load is
easy to generate, for example by setting a snapshot and then
immediately unpacking an archive into the volume.&nbsp; Required IO
bandwidth will triple, seeking between the origin and snapshot store
will increase, and metadata updating will increase.&nbsp; Writing in
this case should be largely linear and batching amortizes the seeking
overhead, so the dominant effect is expected to be the increased IO
bandwidth.&nbsp; For this load we should expect to see a 3 times
slowdown versus raw volume access.&nbsp; Fragmentation of the snapshot
store could make this considerably worse, perhaps by another factor of
three.<br>
<br>
Since such a load is easy to generate it is worrisome.&nbsp; It is
possible that in the long run, general performance for a snapshot
volume could become better than for the origin, see below.<br>
<br>
Fragmentation of the snapshot store will introduce additional seeking
and rotational latency penalties.&nbsp; Reducing such fragmentation by
clever snapshot store allocation policy will yield significant
performance gains, however such allocation policy improvements require
considerable time to develop.&nbsp; A highly fragmented snapshort store
could aggrate worst case write performance by an additional factor of a
few hundred percent.<br>
<br>
[what about latency]<br>
<h2><a class="mozTocH2" name="mozTocId245882"></a>Random Origin Write
Performance</h2>
A load that consists of 100% single-sector writes distributed randomly
over the entire volume immediately after setting a snapshot will cause
copy-out bandwidth to be much more than 200% of raw write bandwidth,
and will also cause a great deal of additional seeking.&nbsp; Metadata
overhead will also increase significantly since typically only a single
chunk on each leaf node will be updated each time the node is
journalled to disk; rotational latency will increase significantly
during metadata access.&nbsp; Performance under this random load will
typically be dominated by seeking rather than bandwidth.&nbsp; Analysis
is complex, however I will speculate now that the performance of the
snapshotted volume could degrade by a factor of 3 to 4 versus the raw
volume due to additional seeking and rotational latency for copy-outs
and metadata updating.<br>
<br>
Fragmentation of the snapshot store can and should be addressed over
time.&nbsp; For origin writes, nothing that can be done about the
copy-out
overhead.&nbsp;&nbsp; Snapshot writes on the other hand do not incurr
copy-out
overhead.&nbsp; They do incurr seeking and rotational penalties due to
fragmentation in the snapshot store, but so do origin writes.&nbsp;
Furthermore snapshot reads also suffer from fragmentation penalties
whereas origin reads do not.&nbsp; Very good snapshot store layout
optimization could reduce both the penalty for snapshot reading and
writing, in which case&nbsp; general performance on a snapshot volume
could
be better than on a snapshotted origin volume.&nbsp; Whether this can
be
realized in practice remains to be seen.<br>
<br>
[what about latency]<br>
<h2><a class="mozTocH2" name="mozTocId983353"></a>Snapshot Read
Performance</h2>
Unlike origin reads, snapshot read throughput is affected by snapshot
store fragmentation.&nbsp; Snapshot read latency is increased by the
requirement of locking against origin writes.&nbsp; Readahead results
in a kind of lockahead, so under loads where readahead is effective,
increased snapshot read latency will not hurt read throughput.&nbsp;
The predominant visible effect is expected to be read
fragmentation.&nbsp; With large chunk sizes, e.g., 256K and up,
moderate fragmentation should cause only slight degradation in snapshot
read performance.&nbsp; However, without special attention to snapshot
store allocation policy, fragmentation can be expected to be fairly
severe, so snapshot read performance is not expected to be steller in
early implementations.&nbsp; Fortunately, since the main purpose of
reading from a snapshot is to back it up or restore a few files, some
read performance degradation is acceptable and is unlikely to be
noticed.<br>
<br>
In the long run it is desireable to improve snapshot read performance
by controlling snapshot store fragmentation as much as possible, in
order to take advantage of the inherently superior performance of&nbsp;
snapshot writing versus origin writing.<br>
<h2><a class="mozTocH2" name="mozTocId713768"></a>Snapshot Write
Performance</h2>
Snapshot writes to not require copy-outs; if an origin chunk or shared
snapshot store chunk needs to be written, the logical chunk is first
remapped to a new chunk in the snapshot store.&nbsp; With some tweaking
of the message protocol, writing to the chunk could procede as soon as
the new allocation is known, in parallel with the logging of the new
exception.&nbsp; So snapshot writes are inherently quite efficient.<br>
<br>
Snapshot write overhead comes from metadata update overhead and
snapshot store fragmentation.&nbsp; The former is supposed to be small,
on the order of a few percent.&nbsp; The latter could be very large,
and probably will be in initial implementation, perhaps on the order of
a factor of 10.&nbsp; Larger chunk sizes will reduced this seeking
overhead, roughly linearly with the chunk size.&nbsp; Careful layout
optimization could conceivably reduce this to a few percent, even with
small chunks.&nbsp; We shall see.<br>
<h2><a class="mozTocH2" name="mozTocId899929"></a>Network Performance</h2>
The amount of message data needed for each chunk is small, especially
since the message format is designed from the outset to handle ranges
of chunks and multiple ranges in each message.&nbsp; Except for
snapshot reads, each message&nbsp; sequence is only two messages long
(note: approximately.&nbsp; Server responses do not correspond exactly
requests; e.g., any unshared chunks can be acknowledged
immediately).&nbsp; Message traffic is expected to be less than 1% of
disk array traffic.&nbsp; Assuming that the general purpose network
interconnect and storage array interconnect have similar bandwidth,
this is where the expectation that this architecture will scale
linearly to about 100 clients comes from.<br>
<br>
[details]<br>
<h2><a name="mozTocId163638" class="mozTocH2"></a>Overall Performance</h2>
It is expected that typical usage of a snapshotted origin volume will
show only slight reduction of performance versus the raw origin volume,
due to reading being more common than writing.&nbsp; Rewriting chunks
is
optimized by the client's bitmap cache, which is compact and probably
capable of caching all the hot spots of a volume, even for large
volumes.&nbsp; So rewriting should show now visible degradation.&nbsp;
The
performance of fresh writes to snapshotted chunks will degrade
significantly, due to copy-out bandwidth, and to snapshot store
fragmentation, that latter being subject to optimization while the
former is unavoidable.&nbsp; In general, more frequent snapshots cause
more
fresh writes, with the frequency of fresh writes peaking just after the
snapshot and declining over time, till the next snapshot.<br>
<br>
So: what will be the balance of fresh writes vs reads and
rewrites?&nbsp;
How frequently will we see will we see the balance shift for a short
time in the direction of the worst case?&nbsp; How bad is the worst
case?&nbsp;&nbsp;
How likely is it that the user will notice the shifts in write
performance?&nbsp;&nbsp;&nbsp; These all await measurement under live
loads.&nbsp; However
at this point I will speculate that even a relatively&nbsp; early
implementation will show average performance degradation versus a raw
volume of less than ten percent, and that, at worst, performance
degradation will be limited to a factor of four or so just after a
snapshot.&nbsp; For many users, and particularly enterprise users, the
benefits of snapshotting will outweigh the performance loss: it is easy
to buy bandwidth, not as easy to buy live backup
capability.&nbsp;&nbsp; For
others, the unavoidable performance degradation of origin writing will
make snapshotting unattractive enough to discourage its use.&nbsp;
Eventually we may be able to satisfy this group as well, by improving
snapshot store allocation policy to the point where the origin can be
made optional and all IO take place in the snapshot store.<br>
<br>
The pessimism in this section should be tempered by observing that in
many respects, performance is expected to be good:<br>
<ul>
  <li>Large number of snapshots can be held without affecting
performance much</li>
  <li>Snapshot store utilization is good</li>
  <li>Network traffic is minimal</li>
  <li>Rewrites are highly optimized</li>
</ul>
In other words, if you need snapshots then this implementation is
likely to deliver good performance versus alternatives.&nbsp;&nbsp;
Plus there is
a clear path forward to achieving near-optimal performance, by working
towards a system where the snapshot store can be used effectively
alone, with no origin volume
<h1><a class="mozTocH1" name="mozTocId895406"></a>Parallelizing the
Architecture</h1>
Normally the first question I am asked about this clustered snapshot
design is "why isn't it symmetric"?&nbsp; The answer: because a) it
doesn't have to be in order to perform well on today's typical clusters
and b) distributing a tree structure across independent caches is a
complex, error-prone process, and introduces overhead of its own.&nbsp;
At some point, however, the single node server architecture will become
a bottleneck, so I discuss parallelizing strategies here.<br>
<br>
The easist thing we can do, and with the strongest immediate effect is
to have the server distribute the copy-out work to underused
nodes.&nbsp; This will take significant IO bandwidth load off the
server's bus at the expense of a little messaging latency.&nbsp; By
doing this, a single server can likely scale to handle a hundred or so
busy nodes of similar power to itself: the real bottleneck will
probably be the storage array.&nbsp; A user who can afford to upgrade
the storage array to handle even larger numbers of clients can likely
afford to upgrade the snapshot server as well.<br>
<br>
At some point, perhaps two or three hundred clients, the snapshot
server
becomes a bottleneck again.&nbsp; Further scaling is easily achieved by
dividing up the work between a number of snapshot servers, by logical
address range.&nbsp; Each snapshot server maintains a separate btree in
a distinct range of logical addresses and operates its own
journal.&nbsp; Care must be taken that allocation bitmaps are divided
up cleanly; this is not hard (e.g., even if a logical address range
boundary lies in the middle of a bitmap block, the boundary bitmap can
be replicated between two nodes, with logic to prevent allocation
outside the boundary - needed anyway for error checking).&nbsp; Shared
metadata such as the current snapshot list, superblock, etc., is
updated using a RW locking strategy (i.e., using a DLM).&nbsp; Assuming
that workload is distributed relatively evenly across the logical
address range, this simple parallelization strategy will serve up to a
thousand clients or so, and the disk will once again be the bottleneck.<br>
<br>
[It might be best to add the metadata hooks for this range division now
since we know it's needed eventually]<br>
<br>
If we want to scale to far larger numbers of cients&nbsp; we probably
have to bite the bullet and distribute the btrees and allocation
bitmaps.&nbsp; However I do not think this problem is imminent; there
is plenty of time to think about it.<br>
<h1><a class="mozTocH1" name="mozTocId426297"></a>Adaptation to Single
Node Client</h1>
The current device-mapper snapshot design suffers from a number of
drawbacks, chiefly<br>
<ul>
  <li>copy-out overhead increases linearly with number of snapshots held</li>
  <li>snapshot state is not recorded durably, but only at shutdown</li>
  <li>all metadata is held in memory, creating excessive cache pressure
for large volumes</li>
</ul>
It is therefore expected that this design for clustered snapshots will
be adapted sooner rather than later for use with the normal,
non-clustered machines that constitute the vast majority of Linux
installs.&nbsp; How best to do that?<br>
<br>
The message-based synchronization described above may not be the
optimal solution for an entirely local implementation.&nbsp; But then,
with some tweaking it just might be.&nbsp; Currently I am considering
the wisdom of adapting the clustered snapshot target for local use by
replacing the socket messaging with a lightweight ring buffer messaging
facility that presents the same interface to the rest of the
target.&nbsp; The obvious alternative is to incorporate the server
operations directly into the client.&nbsp; It's clear which is easier,
but which is better?&nbsp; [feedback invited]<br>
</body>
</html>
