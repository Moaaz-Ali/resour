diff -up --recursive 2.6.7.csnap.clean/drivers/md/Kconfig 2.6.7.csnap/drivers/md/Kconfig
--- 2.6.7.csnap.clean/drivers/md/Kconfig	2004-06-16 05:18:58.000000000 +0000
+++ 2.6.7.csnap/drivers/md/Kconfig	2004-08-30 04:36:36.000000000 +0000
@@ -180,5 +180,15 @@ config DM_CRYPT
 
 	  If unsure, say N.
 
+config DM_CSNAP
+	tristate "Cluster snapshot target support"
+	depends on BLK_DEV_DM && EXPERIMENTAL
+	---help---
+	  This device-mapper target allows you to create a virtual device
+	  that can take snapshots of an underlying device.  This device
+	  can be accessed simultaneously by multiple nodes of a cluster.
+
+	  If unsure, say N.
+
 endmenu
 
diff -up --recursive 2.6.7.csnap.clean/drivers/md/Makefile 2.6.7.csnap/drivers/md/Makefile
--- 2.6.7.csnap.clean/drivers/md/Makefile	2004-06-16 05:19:35.000000000 +0000
+++ 2.6.7.csnap/drivers/md/Makefile	2004-08-30 04:36:36.000000000 +0000
@@ -24,6 +24,7 @@ obj-$(CONFIG_MD_MULTIPATH)	+= multipath.
 obj-$(CONFIG_BLK_DEV_MD)	+= md.o
 obj-$(CONFIG_BLK_DEV_DM)	+= dm-mod.o
 obj-$(CONFIG_DM_CRYPT)		+= dm-crypt.o
+obj-$(CONFIG_DM_CSNAP)		+= dm-csnap.o
 
 quiet_cmd_unroll = UNROLL  $@
       cmd_unroll = $(PERL) $(srctree)/$(src)/unroll.pl $(UNROLL) \
diff -up --recursive 2.6.7.csnap.clean/drivers/md/dm-csnap.c 2.6.7.csnap/drivers/md/dm-csnap.c
--- 2.6.7.csnap.clean/drivers/md/dm-csnap.c	2004-08-31 17:46:41.000000000 +0000
+++ 2.6.7.csnap/drivers/md/dm-csnap.c	2004-09-24 04:37:35.000000000 +0000
@@ -0,0 +1,1020 @@
+#include <linux/fs.h>
+#include <linux/slab.h>
+#include <linux/mm.h>
+#include <linux/module.h>
+#include <linux/pagemap.h>
+#include <linux/file.h>
+#include <linux/syscalls.h> // recvmsg
+#include <net/sock.h>
+#include <asm/uaccess.h>
+#include <linux/bio.h>
+#include "dm.h"
+#include "dm-csnap.h"
+
+#define BREAK BUG()
+#define warn(string, args...) do { printk("%s: " string "\n", __func__, ##args); } while (0)
+#define error(string, args...) do { warn(string, ##args); BREAK; } while (0)
+#define assert(expr) do { if (!(expr)) error("Assertion " #expr " failed!\n"); } while (0)
+#define trace_on(args) args
+#define trace_off(args)
+
+#define trace trace_on
+
+/* Pipe helpers */
+
+static int rwpipe(struct file *file, const void *buffer, unsigned int count,
+	ssize_t (*op)(struct kiocb *, const char *, size_t, loff_t), int mode)
+{
+	struct kiocb iocb;
+	mm_segment_t oldseg;
+	int err = 0;
+
+	trace_off(warn("%s %i bytes", mode == FMODE_READ? "read": "write", count);)
+	if (!(file->f_mode & mode))
+		return -EBADF;
+	if (!op)
+		return -EINVAL;
+	init_sync_kiocb(&iocb, file); // new in 2.5 (hmm)
+	iocb.ki_pos = file->f_pos;
+	oldseg = get_fs();
+	set_fs(get_ds());
+	while (count) {
+		int chunk = (*op)(&iocb, buffer, count, iocb.ki_pos);
+		if (chunk <= 0) {
+			err = chunk? chunk: -EPIPE;
+			break;
+		}
+		BUG_ON(chunk > count);
+		count -= chunk;
+		buffer += chunk;
+	}
+	set_fs(oldseg);
+	file->f_pos = iocb.ki_pos;
+	return err;
+}
+
+static inline int readpipe(struct file *file, void *buffer, unsigned int count)
+{
+	return rwpipe(file, buffer, count, (void *)file->f_op->aio_read, FMODE_READ);
+}
+
+static inline int writepipe(struct file *file, void *buffer, unsigned int count)
+{
+	return rwpipe(file, buffer, count, file->f_op->aio_write, FMODE_WRITE);
+}
+
+#define outbead(SOCK, CODE, STRUCT, VALUES...) ({ \
+	struct { struct head head; STRUCT body; } PACKED message = \
+		{ { CODE, sizeof(STRUCT) }, { VALUES } }; \
+	writepipe(SOCK, &message, sizeof(message)); })
+
+
+typedef u64 chunk_t;
+
+#define SECTOR_SHIFT 9
+#define IS_SNAP_FLAG 1
+#define CONNECTED_FLAG 2 // not actually used at the moment
+#define FINISH_FLAG 4
+#define NUM_BUCKETS 64
+#define MASK_BUCKETS (NUM_BUCKETS - 1)
+#define ID_BITS 16
+
+struct snapinfo {
+	unsigned flags;
+	unsigned chunksize_bits;
+	unsigned chunkshift;
+	sector_t len;
+	int snap, nextid;
+
+	u32 *shared_bitmap; // !!! get rid of this, use the inode cache
+	struct inode  *inode; /* the cache */
+	struct dm_dev *orgdev;
+	struct dm_dev *snapdev;
+	struct file *sock;
+	struct file *control_socket;
+	struct semaphore imutex;
+	struct semaphore omutex;
+	struct semaphore exit1_sem;
+	struct semaphore exit2_sem;
+	struct semaphore exit3_sem;
+	struct semaphore work_sem;
+	struct list_head pending_buckets[NUM_BUCKETS];
+	struct list_head read_release_list;
+	spinlock_t pending_lock;
+};
+
+static inline int is_snapshot(struct snapinfo *info)
+{
+	return !!(info->flags & IS_SNAP_FLAG);
+}
+
+static inline int connected(struct snapinfo *info)
+{
+	return !!(info->flags & CONNECTED_FLAG);
+}
+
+static inline int running(struct snapinfo *info)
+{
+	return !(info->flags & FINISH_FLAG);
+}
+
+/* Static caches, shared by all csnap instances */
+
+static kmem_cache_t *pending_cache;
+static kmem_cache_t *end_io_cache;
+static struct super_block *snapshot_super;
+
+/* We cache query results because we are greedy about speed,  */
+
+#ifdef CACHE
+static u64 *snap_map_cachep(struct address_space *mapping, chunk_t chunk, struct page **p)
+{
+	u32 page_index;
+	u32 page_pos;
+	struct page *page;
+	u64 *exceptions;
+
+	page_index = chunk / (PAGE_SIZE / sizeof(u64));
+	page_pos = chunk % (PAGE_SIZE / sizeof(u64));
+
+	page = find_or_create_page(mapping, page_index, GFP_KERNEL);
+	if (page) {
+		/* Clean page if it's a new one */
+		if (!Page_Uptodate(page)) {
+			memset(page_address(page), 0, PAGE_SIZE);
+			SetPageUptodate(page);
+		}
+
+		exceptions = page_address(page);
+		*p = page;
+		return &exceptions[page_pos];
+	}
+	return NULL;
+}
+
+static inline int get_unshared_bit(struct snapinfo *info, chunk_t chunk)
+{
+	return (info->shared_bitmap[chunk >> 5] >> (chunk & 31)) & 1;
+}
+
+static inline void set_unshared_bit(struct snapinfo *info, chunk_t chunk)
+{
+	info->shared_bitmap[chunk >> 5] |= 1 << (chunk & 31);
+}
+#endif
+
+/* Hash table matches up query replies to pending requests */
+
+struct pending {
+	u64 chunk;
+	unsigned chunks;
+	unsigned id;
+	struct bio *bio;
+	struct list_head list;
+};
+
+static void show_pending(struct snapinfo *info)
+{
+	unsigned h, total = 0;
+
+	spin_lock(&info->pending_lock);
+	warn("");
+	for (h = 0; h < NUM_BUCKETS; h++) {
+		struct list_head *list;
+
+		list_for_each(list, info->pending_buckets + h) {
+			struct pending *pending = list_entry(list, struct pending, list);
+			if (!total)
+				printk("[%u]: ", h);
+			printk("%u:%Lx ", pending->id, pending->chunk);
+			total++;
+		}
+	}
+	printk("(%u)\n", total);
+	spin_unlock(&info->pending_lock);
+}
+
+static unsigned int hash_pending(unsigned id)
+{
+	return id & MASK_BUCKETS;
+}
+
+static struct pending *alloc_pending(void)
+{
+	struct pending *p = kmem_cache_alloc(pending_cache, GFP_NOIO);
+	if (!p)
+		p = kmem_cache_alloc(pending_cache, GFP_ATOMIC);
+	return p;
+}
+
+// !!! used in only one place, put it there
+static struct pending *find_pending(struct snapinfo *info, unsigned id)
+{
+	struct list_head *list, *bucket = info->pending_buckets + hash_pending(id);
+	struct pending *p;
+
+	list_for_each(list, bucket)
+		if ((p = list_entry(list, struct pending, list))->id == id)
+			goto found;
+	p = NULL;
+found:
+	return p;
+}
+
+/* Ah, now it gets interesting.  Called in interrupt context */
+
+struct end_io_hook {
+	struct snapinfo *info;
+	sector_t sector;
+	/* needed only for end_io, make it a union */
+	bio_end_io_t *old_end_io;
+	void *old_private;
+	/* needed after end_io, for release, make it a union */
+	struct list_head list;
+};
+
+static int snapshot_read_end_io(struct bio *bio, unsigned int done, int error)
+{
+	struct end_io_hook *hook = bio->bi_private;
+	struct snapinfo *info = hook->info;
+
+	trace(warn("sector %Lx", (long long)hook->sector);)
+	spin_lock(&info->pending_lock);
+	list_add(&hook->list, &info->read_release_list);
+	spin_unlock(&info->pending_lock);
+	up(&info->work_sem);
+
+	bio->bi_private = hook->old_private;
+	bio->bi_end_io = hook->old_end_io;
+	return bio->bi_end_io(bio, done, error);
+}
+
+/* This is the part that does all the work. */
+
+void kick(struct block_device *dev)
+{
+	request_queue_t *q = bdev_get_queue(dev);
+	if (q->unplug_fn)
+		q->unplug_fn(q);
+}
+
+int replied_rw(struct dm_target *target, struct rw_request *body, unsigned length, int rw, int snap)
+{
+	struct snapinfo *info = target->private;
+	struct chunk_range *p = body->ranges;
+	unsigned shift = info->chunksize_bits - SECTOR_SHIFT, mask = (1 << shift) - 1;
+	int i, j, submitted = 0;
+
+	trace(show_pending(info);)
+	trace(warn("id = %u, %u ranges, %s %s", body->id, body->count,
+		rw == READ? "read from": "write to", snap? "snapshot": "origin");)
+
+	for (i = 0; i < body->count; i++) { // !!! check for length overrun
+		u64 chunk = p->chunk;
+		unsigned chunks = p->chunks;
+		struct pending *pending;
+		struct bio *bio;
+
+		trace(warn("[%Lx/%x]", chunk, chunks);)
+		assert(chunks == 1);
+
+		spin_lock(&info->pending_lock);
+		if (!(pending = find_pending(info, body->id))) {
+			warn("Can't find pending rw for chunk %u:%Lx", body->id, chunk);
+			return -1;
+		}
+		list_del(&pending->list);
+		spin_unlock(&info->pending_lock);
+
+		bio = pending->bio;
+		warn("Handle pending IO sector %Lx", (long long)bio->bi_sector);
+
+		if (chunks != pending->chunks) {
+			warn("Message mismatch, expected %x got %x", chunks, chunks);
+			kmem_cache_free(pending_cache, pending);
+			bio_io_error(bio, bio->bi_size);
+			return -1;
+		}
+
+		++p;
+		if (snap) {
+			chunk_t *p2 = (chunk_t *)p;
+			for (j = 0; j < chunks; j++) {
+				u64 physical = (*p2++ << shift) + (bio->bi_sector & mask);
+				trace(warn("logical %Lx = physical %Lx", (u64)bio->bi_sector, physical));
+				bio->bi_bdev = info->snapdev->bdev;
+				bio->bi_sector = physical;
+			}
+			p = (struct chunk_range *)p2;
+		} else if (rw == READ) {
+			/* snapshot read from origin */
+			struct end_io_hook *hook;
+			trace(warn("hook end_io for %Lx", (long long)bio->bi_sector));
+			if (!(hook = kmem_cache_alloc(end_io_cache, GFP_KERNEL))) {
+				// !!! err out the IO
+				return -1;
+			}
+			*hook = (struct end_io_hook){
+				.info = info,
+				.sector = bio->bi_sector,
+				.old_end_io = bio->bi_end_io,
+				.old_private = bio->bi_private };
+			bio->bi_end_io = snapshot_read_end_io;
+			bio->bi_private = hook;
+		}
+
+		generic_make_request(bio);
+		submitted++;
+#ifdef CACHE
+		for (j = 0; j < p->chunks; j++)
+			set_unshared_bit(info, chunk + j);
+#endif
+		kmem_cache_free(pending_cache, pending);
+	}
+	if (submitted){
+		kick(info->orgdev->bdev);
+		kick(info->snapdev->bdev);
+	}
+	return 0;
+}
+
+/*
+ * There happen to be four flavors of server replies to rw queries, two
+ * write and two read, but the symmetry ends there.  Only one flavor
+ * (write) is for origin IO, because origin reads do not need global
+ * synchronization.  The remaining three flavors are for snapshot IO.
+ * Snapshot writes are always to the snapshot store, so there is only
+ * one flavor.  On the other hand, snapshot reads can be from either
+ * the origin or the snapshot store.  Only the server can know which.
+ * Either or both kinds of snapshot read reply are possible for a given
+ * query, which is where things get nasty.  These two kinds of replies
+ * can be interleaved arbitrarily along the original read request, and
+ * to just to add a little more spice, the server may not send back the
+ * results for an entire query in one message (it may decide to service
+ * other queries first, or replly about the 'easiest' chunks first). The
+ * client has to match up all these reply fragments to the original
+ * request and decide what to do.  Such bizarre fragmentation of the
+ * incoming request is unavoidable, it results from write access
+ * patterns to the origin.  We just have to grin and deal with it.  So
+ * without further ado, here is how the various reply flavors
+ *
+ * - Origin write replies just have logical ranges, since origin physical 
+ *   address is the same as logical.
+ *
+ * - Snapshot read replies come back in two separate messages, one for
+ *   the origin reads (if any) and one for the snapstore reads (if any),
+ *   the latter includes snapstore addresses.  Origin reads are globally
+ *   locked by the server, so we must send release messages on
+ *   completion.
+ *
+ * - Snapshot writes are always to the snapstore, so snapstore write
+ *   replies always include snapstore addresses.
+ *
+ * We know whether we're supposed to be a snapshot or origin client,
+ * but we only use that knowledge as a sanity check.  The message codes
+ * tell us explicitly whether the IO target is origin or snapstore.
+ */
+
+/*
+ * For now, we just block on incoming message traffic, so this daemon
+ * can't do any other useful work.  It could if we used nonblocking pipe
+ * IO but we have been too lazy to implement it so far.  So we have one
+ * more daemon than we really need, and maybe we will get energetic one
+ * day soon and get rid of it.
+ *
+ * When it comes time to destroy things, the daemon has to be kicked
+ * out of its blocking wait, if it is in one, which it probably is.  We
+ * do that by shutting down the socket.  This unblocks the waiters and
+ * feeds them errors.  Does this work for all flavors of sockets?  I
+ * don't know.  It obviously should, but we've seen some pretty silly
+ * limitations in our long life, so nothing would surprise us at this
+ * point.
+ */
+static int incoming(struct dm_target *target)
+{
+	struct snapinfo *info = target->private;
+	struct messagebuf message; // !!! have a buffer in the target->info
+	struct file *sock;
+	struct task_struct *task = current;
+	int err, length;
+
+	strcpy(task->comm, "csnap-client");
+	trace(warn("Client thread started, pid=%i", current->pid);)
+	down(&info->imutex);
+	trace(warn("got socket %p", info->sock);)
+	sock = info->sock;
+
+	down(&info->exit2_sem);
+	while (running(info)) { // stop on module exit
+		int rw, to_snap;
+
+		trace(warn("wait message");)
+		if ((err = readpipe(sock, &message.head, sizeof(message.head))))
+			goto socket_error;
+		length = message.head.length;
+		if (length > maxbody)
+			goto message_too_long;
+		trace(warn("%x/%u", message.head.code, length);)
+		if ((err = readpipe(sock, &message.body, length)))
+			goto socket_error;
+	
+		switch (message.head.code) {
+		case REPLY_ORIGIN_WRITE:
+			rw = WRITE;
+			to_snap = 0;
+			break;
+
+		case REPLY_SNAPSHOT_WRITE:
+			rw = WRITE;
+			to_snap = 1;
+			break;
+
+		case REPLY_SNAPSHOT_READ_ORIGIN:
+			rw = READ;
+			to_snap = 0;
+			break;
+
+		case REPLY_SNAPSHOT_READ:
+			rw = READ;
+			to_snap = 1;
+			break;
+
+		case REPLY_CREATE_SNAPSHOT:
+			trace(warn("create snapshot succeeded");)
+			continue;
+
+		case REPLY_IDENTIFY:
+			trace(warn("identify succeeded");)
+			up(&info->omutex);
+			continue;
+
+		default: 
+			warn("Unknown message %x", message.head.code);
+			continue;
+		}
+		if (length < sizeof(struct rw_request))
+			goto message_too_short;
+
+		replied_rw(target, (void *)message.body, length, rw, to_snap);
+	}
+out:
+	up(&info->exit2_sem); /* !!! will crash if module unloaded before ret executes */
+	warn("%s exiting", task->comm);
+	return 0;
+message_too_long:
+	warn("message %x too long (%u bytes)", message.head.code, message.head.length);
+	goto out;
+message_too_short:
+	warn("message %x too short (%u bytes)", message.head.code, message.head.length);
+	goto out;
+socket_error:
+	warn("socket error %i", err);
+	goto out;
+}
+
+#if 0
+/*
+ * This is disgusting, but there's no standard sock->ops method for getting it
+ */
+int recv_available(struct socket *sock)
+{
+	int bytes, err;
+	mm_segment_t oldfs = get_fs();
+	set_fs(get_ds());
+	err = sock->ops->ioctl(sock, SIOCINQ, (unsigned long)&bytes);
+	set_fs(oldfs);
+	return (err >= 0)? bytes: err;
+}
+// now, this doesn't fit the model of the worker thread bacause there's no
+// obvious way to kick it into action, and it doesn't fit the model of the
+// incoming thread because
+#endif
+
+/*
+ * Here is our nonblocking worker daemon.  It handles all events other
+ * than incoming socket traffic.  At the moment, its only job is to
+ * send read release messages that can't be sent directly from the read
+ * end_io function, which executes in interrupt context.  But soon its
+ * duties will be expanded to include submitting IO that was blocked
+ * because no server pipe is connected yet, or something broke the
+ * pipe.  It may also have to resubmit some server queries, if the
+ * server dies for some reason and a new one is incarnated to take its
+ * place.  We also want to check for timed-out queries here.  Sure, we
+ * have heartbeating in the cluster, but why not have the guy who knows
+ * what to expect do the checking.  When we do detect timeouts, we will
+ * punt the complaint upstairs using some interface that hasn't been
+ * invented yet, because nobody has thought too deeply about what you
+ * need to do, to detect faults really quickly and reliably.
+ *
+ * We throttle this daemon using a counting semapore: each up on the
+ * semaphore causes the daemon to loop through its polling sequence
+ * once.  So we make sure we up the daemon's semaphore every time we
+ * queue an event.  The daemon may well process more than one event per
+ * cycle (we want that, actually, because then it can do some, e.g.,
+ * message batching if it wants to) and will therefore end up looping
+ * a few times without doing any work.  This is harmless, and much much
+ * less nasty than missing an event.  When there are no pending events,
+ * the daemon sleeps peacefully.  Killing the daemon is easy, we just
+ * pull down the running flag and up the work semaphore, which causes
+ * our faithful worker to drop through the floor.
+ */
+static int worker(struct dm_target *target)
+{
+	struct snapinfo *info = target->private;
+	struct task_struct *task = current;
+	trace(unsigned events = 0;)
+
+	strcpy(task->comm, "csnap-worker");
+	trace(warn("Worker thread started, pid=%i", current->pid);)
+	down(&info->exit1_sem);
+	while (running(info)) {
+		down(&info->work_sem);
+
+		/* Send message for each pending read release. */
+		spin_lock(&info->pending_lock);
+		while (!list_empty(&info->read_release_list)) {
+			struct list_head *entry = info->read_release_list.prev;
+			struct end_io_hook *hook = list_entry(entry, struct end_io_hook, list);
+			chunk_t chunk = hook->sector >> info->chunkshift;
+
+			list_del(entry);
+			spin_unlock(&info->pending_lock);
+			trace(warn("%u. release sector %Lx, chunk %Lx", ++events, (long long)hook->sector, chunk);)
+			kmem_cache_free(end_io_cache, hook);
+			down(&info->omutex);
+			outbead(info->sock, FINISH_SNAPSHOT_READ, struct rw_request1,
+				.count = 1, .ranges[0].chunk = chunk, .ranges[0].chunks = 1);
+			up(&info->omutex);
+			spin_lock(&info->pending_lock);
+		}
+		spin_unlock(&info->pending_lock);
+
+		// * send any messages waiting for socket connection
+		// * resent any messages needing retries
+		// * check for timed-out messages
+		// * check for control messages
+		trace(warn("Yowza! More work?");)
+	}
+	up(&info->exit1_sem); /* !!! crashes if module unloaded before ret executes */
+	warn("%s exiting", task->comm);
+	return 0;
+}
+
+long recvmsg(int sock, struct msghdr *msg, unsigned flags)
+{
+	int err;
+	mm_segment_t oldseg;
+	oldseg = get_fs();
+	set_fs(get_ds());
+	err = sys_recvmsg(sock, msg, flags);
+	set_fs(oldseg);
+	return err;
+}
+
+int recv_fd(int sock, char *tag, unsigned *len)
+{
+	char payload[CMSG_SPACE(sizeof(int))];
+	struct msghdr msg = {
+		.msg_control = payload,
+		.msg_controllen = sizeof(payload),
+		.msg_iov = &(struct iovec){ .iov_base = tag, .iov_len = *len },
+		.msg_iovlen = 1,
+	};
+	struct cmsghdr *cmsg;
+	int err;
+
+	if ((err = recvmsg(sock, &msg, 0)) < 0)
+		return err;
+	*len = err;
+
+	// looping through the control messages is gratuitous and wrong
+	// for this application, it's just a demo of CMSG_NXTHDR
+	for (cmsg = CMSG_FIRSTHDR(&msg); cmsg; cmsg = CMSG_NXTHDR(&msg, cmsg)) {
+		if (cmsg->cmsg_len != CMSG_LEN(sizeof(int)) ||
+			cmsg->cmsg_level != SOL_SOCKET ||
+			cmsg->cmsg_type  != SCM_RIGHTS)
+			continue;
+		return *((int *)CMSG_DATA(cmsg));
+	}
+	return 0;
+}
+
+/*
+ * Yikes, a third daemon, that makes four including the user space
+ * monitor.  This daemon proliferation is due to not using poll, which
+ * we should fix at some point.  Or maybe we should wait for aio to
+ * work properly for sockets, and use that instead.  Either way, we
+ * can combine the two socket-waiting daemons into one, which will look
+ * nicer in ps.  Practically speaking, it doesn't matter a whole lot
+ * though, if we just stay lazy and have too many daemons.
+ */
+static int control(struct dm_target *target)
+{
+	struct snapinfo *info = target->private;
+	struct messagebuf message; // !!! have a buffer in the target->info
+	struct file *sock = info->control_socket;
+	struct task_struct *task = current;
+	int err, length;
+
+	strcpy(task->comm, "csnap-control");
+	trace(warn("Control thread started, pid=%i", current->pid);)
+	trace(warn("got socket %p", info->control_socket);)
+
+	down(&info->exit3_sem);
+	while (running(info)) {
+		trace(warn("wait message");)
+		if ((err = readpipe(sock, &message.head, sizeof(message.head))))
+			goto socket_error;
+		length = message.head.length;
+		if (length > maxbody)
+			goto message_too_long;
+		trace(warn("%x/%u", message.head.code, length);)
+		if ((err = readpipe(sock, &message.body, length)))
+			goto socket_error;
+	
+		switch (message.head.code) {
+		case SERVER_CONNECT: {
+			unsigned len = 4;
+			char tag[len];
+			unsigned sock = recv_fd(sock, tag, &len);
+			warn("Received socket %i", sock);
+//			up(&info->imutex);
+//			outbead(info->sock, IDENTIFY, struct identify, .id = 6, .snap = info->snap);
+			break;
+		}
+		default: 
+			warn("Unknown message %x", message.head.code);
+			continue;
+		}
+		if (length < sizeof(struct rw_request))
+			goto message_too_short;
+	}
+out:
+	up(&info->exit3_sem); /* !!! will crash if module unloaded before ret executes */
+	warn("%s exiting", task->comm);
+	return 0;
+message_too_long:
+	warn("message %x too long (%u bytes)", message.head.code, message.head.length);
+	goto out;
+message_too_short:
+	warn("message %x too short (%u bytes)", message.head.code, message.head.length);
+	goto out;
+socket_error:
+	warn("socket error %i", err);
+	goto out;
+}
+
+/*
+ * This is the device mapper mapping method, which does one of three things:
+ * (1) tells device mapper to go ahead and submit the request with a default
+ * identity mapping (return 1) (2) tells device mapper to forget about the
+ * request (return 0), goes off and does its own thing, or (3) on a bad
+ * day, tells device mapper to fail the IO (return negative errnum).
+ *
+ * This is pretty simple: we just hand any origin reads back to device mapper
+ * after filling in the origin device.  Then, we check the cache to see if
+ * if conditions are right to map the request locally, otherwise we need help
+ * from the server, so we remember the request in the pending hash and send
+ * off the appropriate server query.
+ *
+ * To make this a little more interesting, our server connection may be broken
+ * at the moment, or may not have been established yet, in which case we have
+ * to defer the request until the server becomes available.
+ */
+static int csnap_map(struct dm_target *target, struct bio *bio, union map_info *context)
+{
+	struct snapinfo *info = target->private;
+	struct pending *pending;
+	chunk_t chunk;
+	unsigned id;
+
+	bio->bi_bdev = info->orgdev->bdev;
+	if (bio_data_dir(bio) == READ && !is_snapshot(info))
+		return 1;
+
+	chunk = bio->bi_sector >> info->chunkshift;
+	trace(warn("map %Lx/%x, chunk %Lx", (long long)bio->bi_sector, bio->bi_size, chunk);)
+	assert(bio->bi_size <= 1 << info->chunksize_bits);
+#ifdef CACHE
+	if (is_snapshot(info)) { // !!! use page cache for both
+		struct page *page;
+		u64 *exception = snap_map_cachep(info->inode->i_mapping, chunk, &page);
+	
+		if (!exception) {
+			printk("Failed to get a page for sector %ld\n", bio->bi_sector);
+			return -1;
+		}
+
+		u64 exp_chunk = *exception;
+		UnlockPage(page);
+		if (exp_chunk) {
+			bio->bi_sector = bio->bi_sector + ((exp_chunk - chunk) << info->chunkshift);
+			return 1;
+		}
+	} else {
+		if (info->shared_bitmap && get_unshared_bit(info, chunk))
+			return 1;
+	}
+#endif
+	pending = alloc_pending();
+	if (!pending) {
+		warn("Cannot allocate pending struct");
+		return -1;
+	}
+
+	id = info->nextid;
+	info->nextid = (id + 1) & ~(-1 << ID_BITS);
+	*pending = (struct pending){ .id = id, .bio = bio, .chunk = chunk, .chunks = 1 };
+	spin_lock(&info->pending_lock);
+	list_add(&pending->list, info->pending_buckets + hash_pending(pending->id));
+	spin_unlock(&info->pending_lock);
+	trace(show_pending(info);)
+
+// !!! don't send this inline, the box can deadlock !!!
+//	add_outbound(info, pending);
+
+	down(&info->omutex);
+	// !!! check for dm device removed
+	outbead(info->sock,
+		bio_data_dir(bio) == WRITE? QUERY_WRITE: QUERY_SNAPSHOT_READ,
+		struct rw_request1,
+			.id = pending->id, .count = 1,
+			.ranges[0].chunk = chunk,
+			.ranges[0].chunks = 1);
+
+	up(&info->omutex);
+	return 0;
+}
+
+/*
+ * Carefully crafted not to care about how far we got in the process
+ * of instantiating our client.  As such, it serves both for error
+ * abort and device unload destruction.  We have to scour our little
+ * world for resources and give them all back, including any pending
+ * requests, context structures and daemons.  The latter have to be
+ * convince to exit on demand, and we must be sure they have exited,
+ * so we synchronize that with semaphores.  This isn't 100% foolproof'
+ * there is still the possibility that the destructor could gain
+ * control between the time a daemon ups its exit semaphore and when
+ * it has actually returned to its caller.  In that case, the module
+ * could be unloaded and the exiting thread will segfault.  This is
+ * a basic flaw in Linux that I hope to get around to fixing at some
+ * point, one way or another.
+ */
+static void csnap_destroy(struct dm_target *target)
+{
+	struct snapinfo *info = target->private;
+
+	trace(warn("%p", target);)
+	if (!info)
+		return;
+
+	info->flags |= FINISH_FLAG;
+	up(&info->imutex); // unblock incoming thread
+	up(&info->omutex); // unblock io request threads
+
+	if (info->sock) {
+		struct inode *inode = info->sock->f_dentry->d_inode;
+		struct socket *sock = SOCKET_I(inode);
+		int err = sock->ops->shutdown(sock, RCV_SHUTDOWN);
+		warn("socket shutdown, err=%i", err);
+	}
+	up(&info->work_sem);
+
+	// !!! wrong! the thread might be just starting, think about this some more
+	down(&info->exit1_sem);
+	warn("thread 1 exited");
+	down(&info->exit2_sem);
+	warn("thread 2 exited");
+	down(&info->exit3_sem);
+	warn("thread 3 exited");
+
+	if (info->sock)
+		fput(info->sock);
+	if (info->inode)
+		iput(info->inode);
+	if (info->shared_bitmap)
+		vfree(info->shared_bitmap);
+	if (info->snapdev)
+		dm_put_device(target, info->snapdev);
+	if (info->orgdev)
+		dm_put_device(target, info->orgdev);
+	kfree(info);
+}
+
+/*
+ * Woohoo, we are going to instantiate a new cluster snapshot virtual
+ * device, what fun.
+ */
+
+/*
+ * Round up to nearest 2**k boundary
+ * !!! lose this
+ */
+static inline ulong round_up(ulong n, ulong size)
+{
+	return (n + size - 1) & ~(size - 1);
+}
+
+static int csnap_create(struct dm_target *target, unsigned argc, char **argv)
+{
+	u64 chunksize_bits = 12; // !!! when chunksize isn't always 4K, have to move all this to identify reply handler
+	struct dm_dev *orgdev, *snapdev;
+	struct snapinfo *info;
+	char uuid[64], *error;
+	int err, i, snap, flags = 0;
+	unsigned bm_size;
+
+	trace_on(warn("target = %p", target);)
+	error = "csnap usage: orgdev snapdev snapnum [uuid]";
+	err = -EINVAL;
+	if (argc < 3 || argc > 4)
+		goto eek;
+	error = "Can't get origin device";
+	if ((err = dm_get_device(target, argv[0], 0, target->len, dm_table_get_mode(target->table), &orgdev)))
+		goto eek;
+	error = "Can't get snapshot device";
+	if ((err = dm_get_device(target, argv[1], 0, target->len, dm_table_get_mode(target->table), &snapdev)))
+		goto eek;
+	snap = simple_strtol(argv[2], NULL, 0);
+	if (snap != -1)
+		flags |= IS_SNAP_FLAG;
+
+	err = -EINVAL;
+	if (argc == 4) {
+		error = "UUID too long";
+		if (strlen(argv[3]) > sizeof(uuid))
+			goto eek;
+		strncpy(uuid, argv[3], sizeof(uuid));
+	}
+
+	err = -ENOMEM;
+	if (!(info = kmalloc(sizeof(struct snapinfo), GFP_KERNEL)))
+		goto eek;
+
+	*info = (struct snapinfo){ .flags = flags, .orgdev = orgdev, .snapdev = snapdev, .snap = snap };
+	// these bits can all go in the structure assignment (and the sem inits)...
+	info->len = target->len; // !!! huh? /* so we can create the bitmap when we know the chunk size */
+	info->chunksize_bits = chunksize_bits;
+	info->chunkshift = chunksize_bits - SECTOR_SHIFT;
+	target->private = info;
+
+	bm_size = round_up((info->len  + 7) >> (chunksize_bits + 3), sizeof(u32)); // !!! wrong
+	error = "Can't allocate bitmap for origin";
+	if (!(info->shared_bitmap = vmalloc(bm_size)))
+		goto eek;
+	memset(info->shared_bitmap, 0, bm_size);
+	if (!(info->inode = new_inode(snapshot_super)))
+		goto eek;
+
+	sema_init(&info->imutex, 0);
+	sema_init(&info->omutex, 0);
+	sema_init(&info->exit1_sem, 1);
+	sema_init(&info->exit2_sem, 1);
+	sema_init(&info->exit3_sem, 1);
+	sema_init(&info->work_sem, 0);
+
+	spin_lock_init(&info->pending_lock);
+
+	{
+		struct list_head *bucket = info->pending_buckets;
+		for (i = 0; i < NUM_BUCKETS; i++, bucket++)
+			INIT_LIST_HEAD(bucket);
+	}
+
+	INIT_LIST_HEAD(&info->read_release_list);
+	error = "Can't start daemon";
+	if ((err = kernel_thread((void *)incoming, target, CLONE_KERNEL)) < 0)
+		goto eek;
+	if ((err = kernel_thread((void *)worker, target, CLONE_KERNEL)) < 0)
+		goto eek;
+	warn("Created snapshot device origin=%s snapstore=%s snapshot=%i", argv[0], argv[1], snap);
+	target->split_io = 1 << info->chunkshift; // !!! lose this as soon as possible
+	return 0;
+
+eek:	warn("Virtual device create error %i: %s!", err, error);
+	csnap_destroy(target);
+	target->error = error;
+	return err;
+}
+
+/*
+ * This is a device mapper hook I cooked up for the purpose of connecting
+ * or reconnecting a pipe to the client, for communication to the server.
+ * It really only needs to be able to pass a couple of integers, but it
+ * was scarcely harder to make it handle generic blobs as well, which might
+ * be useful in some other context.  You could even return blobs with this
+ * interface if you wanted too, just call put_user and friends as needed.
+ */
+/*
+ * We're going to lose this soon, and get the control socket at device
+ * create time instead.
+ */
+int csnap_message(struct dm_target *target, int tag, int *msg, int len)
+{
+	struct snapinfo *info = target->private;
+	struct file *sock;
+
+	if (len != sizeof(int) || !(sock = fget(msg[0])))
+		return -EBADF;
+	if (!(sock->f_mode & FMODE_WRITE)) {
+		fput(sock);
+		return -EACCES;
+	}
+	if (info->control_socket)
+		fput(info->control_socket);
+	warn("socket = %p", sock);
+	info->control_socket = sock;
+	return kernel_thread((void *)control, target, CLONE_KERNEL);
+}
+
+/* Is this actually useful?  It's really trying to be a message */
+
+static int csnap_status(struct dm_target *target, status_type_t type, char *result, unsigned int maxlen)
+{
+	char orgbuffer[32];
+	char snapbuffer[32];
+	struct snapinfo *info = target->private;
+
+	switch (type) {
+	case STATUSTYPE_INFO:
+		result[0] = '\0';
+		break;
+
+	case STATUSTYPE_TABLE:
+		format_dev_t(orgbuffer, info->orgdev->bdev->bd_dev);
+		format_dev_t(snapbuffer, info->snapdev->bdev->bd_dev);
+		snprintf(result, maxlen, "%s %s %u",
+			 orgbuffer, snapbuffer, 1 << info->chunksize_bits);
+		break;
+	}
+
+	return 0;
+}
+
+static struct target_type supersnap = {
+	.name = "supersnap",
+	.version = {0, 0, 0},
+	.module = THIS_MODULE,
+	.ctr = csnap_create,
+	.dtr = csnap_destroy,
+	.map = csnap_map,
+	.status = csnap_status,
+	.message = (dm_message_fn)csnap_message,
+};
+
+int __init dm_csnap_init(void)
+{
+	int err;
+
+	trace_on(warn(">>> module init");)
+	if ((err = dm_register_target(&supersnap))) {
+		DMERR("Snapshot register failed %d\n", err);
+		goto bad1;
+	}
+	err = -ENOMEM;
+	pending_cache = kmem_cache_create("csnap-pending", sizeof(struct pending), __alignof__(struct pending), 0, NULL, NULL);
+	if (!pending_cache) {
+		DMERR("Couldn not create pending cache.");
+		goto bad2;
+	}
+	end_io_cache = kmem_cache_create("csnap-pending_r", sizeof(struct end_io_hook), __alignof__(struct end_io_hook), 0, NULL, NULL);
+	if (!end_io_cache) {
+		DMERR("Could not create read pending cache.");
+		goto bad3;
+	}
+	if (!(snapshot_super = alloc_super())) {
+		DMERR("Couldn't create snapshot superblock\n");
+		goto bad4;
+	}
+	return 0;
+
+#ifdef CACHE
+bad5:
+	kfree(snapshot_super);
+#endif
+bad4:
+	kmem_cache_destroy(end_io_cache);
+bad3:
+	kmem_cache_destroy(pending_cache);
+bad2:
+	dm_unregister_target(&supersnap);
+bad1:
+	return err;
+}
+
+void dm_csnap_exit(void)
+{
+	int err;
+	trace_on(warn(">>> module exit");)
+	if ((err = dm_unregister_target(&supersnap)))
+		DMERR("Snapshot unregister failed %d", err);
+	if (pending_cache)
+		kmem_cache_destroy(pending_cache);
+	if (end_io_cache)
+		kmem_cache_destroy(end_io_cache);
+	kfree(snapshot_super);
+}
+
+module_init(dm_csnap_init);
+module_exit(dm_csnap_exit);
diff -up --recursive 2.6.7.csnap.clean/drivers/md/dm-csnap.h 2.6.7.csnap/drivers/md/dm-csnap.h
--- 2.6.7.csnap.clean/drivers/md/dm-csnap.h	2004-08-31 17:46:46.000000000 +0000
+++ 2.6.7.csnap/drivers/md/dm-csnap.h	2004-09-24 00:55:49.000000000 +0000
@@ -0,0 +1,80 @@
+#define PACKED __attribute__ ((packed))
+#define MAGIC  0xadbe
+
+struct head
+{
+	uint32_t code;
+	uint32_t length;
+};
+
+enum csnap_codes
+{
+	REPLY_ERROR = 0xbead0000,
+	IDENTIFY,
+	REPLY_IDENTIFY,
+	QUERY_WRITE,
+	REPLY_ORIGIN_WRITE,
+	REPLY_SNAPSHOT_WRITE,
+	QUERY_SNAPSHOT_READ,
+	REPLY_SNAPSHOT_READ,
+	REPLY_SNAPSHOT_READ_ORIGIN,
+	FINISH_SNAPSHOT_READ,
+	CREATE_SNAPSHOT,
+	REPLY_CREATE_SNAPSHOT,
+	DELETE_SNAPSHOT,
+	REPLY_DELETE_SNAPSHOT,
+	DUMP_TREE,
+	SHUTDOWN_SERVER,
+	INITIALIZE_SNAPSTORE,
+	SERVER_CONNECT,
+};
+
+struct identify { uint32_t id; int32_t snap; } PACKED;
+struct create_snapshot { uint32_t snap; } PACKED;
+
+typedef uint16_t shortcount; /* !!! what is this all about */
+
+struct rw_request
+{
+	uint16_t id;
+	shortcount count;
+	struct chunk_range
+	{
+		uint64_t chunk;
+		shortcount chunks;
+	} PACKED ranges[];
+} PACKED;
+
+/* !!! can there be only one flavor of me please */
+struct rw_request1
+{
+	uint16_t id;
+	shortcount count;
+	struct chunk_range PACKED ranges[1];
+} PACKED;
+
+/* decruft me... !!! */
+#define maxbody 500
+struct rwmessage { struct head head; struct rw_request body; };
+struct messagebuf { struct head head; char body[maxbody]; };
+/* ...decruft me */
+
+/* The endian conversions that libc forgot */
+
+static inline uint64_t ntohll(uint64_t n)
+{
+#if __BYTE_ORDER == __LITTLE_ENDIAN
+	return (((uint64_t)ntohl(n)) << 32) | ntohl(n >> 32);
+#else
+	return n; 
+#endif
+}
+
+static inline uint64_t htonll(uint64_t n)
+{
+#if __BYTE_ORDER == __LITTLE_ENDIAN
+	return (((uint64_t)htonl(n)) << 32) | htonl(n >> 32);
+#else
+	return n; 
+#endif
+}
diff -up --recursive 2.6.7.csnap.clean/drivers/md/dm.c 2.6.7.csnap/drivers/md/dm.c
--- 2.6.7.csnap.clean/drivers/md/dm.c	2004-06-16 05:19:44.000000000 +0000
+++ 2.6.7.csnap/drivers/md/dm.c	2004-08-30 04:36:36.000000000 +0000
@@ -15,6 +15,7 @@
 #include <linux/buffer_head.h>
 #include <linux/mempool.h>
 #include <linux/slab.h>
+#include <linux/dm-ioctl.h>
 
 static const char *_name = DM_NAME;
 #define MAX_DEVICES 1024
@@ -227,6 +228,50 @@ static inline void free_tio(struct mappe
 	mempool_free(tio, md->tio_pool);
 }
 
+
+static int dm_message(struct mapped_device *md, int ith, int tag, void *msg,
+		      int len)
+{
+	int r = -ENXIO;
+	struct dm_table *table;
+	struct dm_target *target;
+
+	if (!(table = dm_get_table(md)))
+		return r;
+	if (!(target = dm_table_get_target(table, ith)))
+		goto out;
+	if (target->type->message)
+		r = target->type->message(target, tag, msg, len);
+out:
+	dm_table_put(table);
+	return r;
+}	
+
+static int dm_blk_ioctl(struct inode *inode, struct file *file,
+			unsigned int cmd, unsigned long a)
+{
+	struct mapped_device *md;
+
+	md = inode->i_bdev->bd_disk->private_data;
+
+	switch (cmd){
+	case DM_MESSAGE: {
+		int head[3];
+		if (!copy_from_user(head, (void *)a, sizeof(head))){
+			int ith = head[0], tag = head[1], len = head[2];
+			char msg[64];
+			if (len <= 64 && !copy_from_user(msg, (int *)a + 3, len))
+				return dm_message(md, ith, tag, msg, len);
+		}
+	}
+	default:
+		DMWARN("unknown block ioctl 0x%x", cmd);
+		return -ENOTTY;
+	}
+
+	return 0;
+}
+
 /*
  * Add the bio to the list of deferred io.
  */
@@ -1082,6 +1127,7 @@ int dm_suspended(struct mapped_device *m
 static struct block_device_operations dm_blk_dops = {
 	.open = dm_blk_open,
 	.release = dm_blk_close,
+	.ioctl = dm_blk_ioctl,
 	.owner = THIS_MODULE
 };
 
diff -up --recursive 2.6.7.csnap.clean/fs/super.c 2.6.7.csnap/fs/super.c
--- 2.6.7.csnap.clean/fs/super.c	2004-06-16 05:19:22.000000000 +0000
+++ 2.6.7.csnap/fs/super.c	2004-08-30 04:36:36.000000000 +0000
@@ -51,7 +51,7 @@ spinlock_t sb_lock = SPIN_LOCK_UNLOCKED;
  *	Allocates and initializes a new &struct super_block.  alloc_super()
  *	returns a pointer new superblock or %NULL if allocation had failed.
  */
-static struct super_block *alloc_super(void)
+struct super_block *alloc_super(void)
 {
 	struct super_block *s = kmalloc(sizeof(struct super_block),  GFP_USER);
 	static struct super_operations default_op;
@@ -87,6 +87,8 @@ out:
 	return s;
 }
 
+EXPORT_SYMBOL(alloc_super);
+
 /**
  *	destroy_super	-	frees a superblock
  *	@s: superblock to free
diff -up --recursive 2.6.7.csnap.clean/include/linux/compat_ioctl.h 2.6.7.csnap/include/linux/compat_ioctl.h
--- 2.6.7.csnap.clean/include/linux/compat_ioctl.h	2004-06-16 05:20:04.000000000 +0000
+++ 2.6.7.csnap/include/linux/compat_ioctl.h	2004-08-30 04:36:36.000000000 +0000
@@ -138,6 +138,7 @@ COMPATIBLE_IOCTL(DM_TABLE_CLEAR_32)
 COMPATIBLE_IOCTL(DM_TABLE_DEPS_32)
 COMPATIBLE_IOCTL(DM_TABLE_STATUS_32)
 COMPATIBLE_IOCTL(DM_LIST_VERSIONS_32)
+COMPATIBLE_IOCTL(DM_MESSAGE_32)
 COMPATIBLE_IOCTL(DM_VERSION)
 COMPATIBLE_IOCTL(DM_REMOVE_ALL)
 COMPATIBLE_IOCTL(DM_LIST_DEVICES)
@@ -152,6 +153,7 @@ COMPATIBLE_IOCTL(DM_TABLE_CLEAR)
 COMPATIBLE_IOCTL(DM_TABLE_DEPS)
 COMPATIBLE_IOCTL(DM_TABLE_STATUS)
 COMPATIBLE_IOCTL(DM_LIST_VERSIONS)
+COMPATIBLE_IOCTL(DM_MESSAGE)
 /* Big K */
 COMPATIBLE_IOCTL(PIO_FONT)
 COMPATIBLE_IOCTL(GIO_FONT)
diff -up --recursive 2.6.7.csnap.clean/include/linux/device-mapper.h 2.6.7.csnap/include/linux/device-mapper.h
--- 2.6.7.csnap.clean/include/linux/device-mapper.h	2004-06-16 05:19:42.000000000 +0000
+++ 2.6.7.csnap/include/linux/device-mapper.h	2004-08-30 04:36:36.000000000 +0000
@@ -56,6 +56,8 @@ typedef void (*dm_resume_fn) (struct dm_
 
 typedef int (*dm_status_fn) (struct dm_target *ti, status_type_t status_type,
 			     char *result, unsigned int maxlen);
+typedef int (*dm_message_fn)(struct dm_target *ti, int tag, void *msg, int len);
+
 
 void dm_error(const char *message);
 
@@ -82,6 +84,7 @@ struct target_type {
 	dm_suspend_fn suspend;
 	dm_resume_fn resume;
 	dm_status_fn status;
+	dm_message_fn message;
 };
 
 struct io_restrictions {
diff -up --recursive 2.6.7.csnap.clean/include/linux/dm-ioctl.h 2.6.7.csnap/include/linux/dm-ioctl.h
--- 2.6.7.csnap.clean/include/linux/dm-ioctl.h	2004-06-16 05:18:38.000000000 +0000
+++ 2.6.7.csnap/include/linux/dm-ioctl.h	2004-08-31 22:07:35.000000000 +0000
@@ -204,6 +204,7 @@ enum {
 
 	/* Added later */
 	DM_LIST_VERSIONS_CMD,
+	DM_MESSAGE_CMD,
 };
 
 /*
@@ -232,6 +233,7 @@ typedef char ioctl_struct[308];
 #define DM_TABLE_DEPS_32    _IOWR(DM_IOCTL, DM_TABLE_DEPS_CMD, ioctl_struct)
 #define DM_TABLE_STATUS_32  _IOWR(DM_IOCTL, DM_TABLE_STATUS_CMD, ioctl_struct)
 #define DM_LIST_VERSIONS_32 _IOWR(DM_IOCTL, DM_LIST_VERSIONS_CMD, ioctl_struct)
+#define DM_MESSAGE_32       _IOWR(DM_IOCTL, DM_MESSAGE_CMD, ioctl_struct)
 #endif
 
 #define DM_IOCTL 0xfd
@@ -253,6 +255,7 @@ typedef char ioctl_struct[308];
 #define DM_TABLE_STATUS  _IOWR(DM_IOCTL, DM_TABLE_STATUS_CMD, struct dm_ioctl)
 
 #define DM_LIST_VERSIONS _IOWR(DM_IOCTL, DM_LIST_VERSIONS_CMD, struct dm_ioctl)
+#define DM_MESSAGE       _IOWR(DM_IOCTL, DM_MESSAGE_CMD, int[3])
 
 #define DM_VERSION_MAJOR	4
 #define DM_VERSION_MINOR	1
diff -up --recursive 2.6.7.csnap.clean/include/linux/fs.h 2.6.7.csnap/include/linux/fs.h
--- 2.6.7.csnap.clean/include/linux/fs.h	2004-06-16 05:19:02.000000000 +0000
+++ 2.6.7.csnap/include/linux/fs.h	2004-08-30 04:36:36.000000000 +0000
@@ -1109,6 +1109,7 @@ void generic_shutdown_super(struct super
 void kill_block_super(struct super_block *sb);
 void kill_anon_super(struct super_block *sb);
 void kill_litter_super(struct super_block *sb);
+struct super_block *alloc_super(void);
 void deactivate_super(struct super_block *sb);
 int set_anon_super(struct super_block *s, void *data);
 struct super_block *sget(struct file_system_type *type,
